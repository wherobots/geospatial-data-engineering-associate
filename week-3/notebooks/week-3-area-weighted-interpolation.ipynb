{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4397f8dc-728a-4de0-87be-e8cc233e3a0a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T01:48:13.445083Z",
     "iopub.status.busy": "2025-10-20T01:48:13.444972Z",
     "iopub.status.idle": "2025-10-20T01:48:14.745314Z",
     "shell.execute_reply": "2025-10-20T01:48:14.744368Z",
     "shell.execute_reply.started": "2025-10-20T01:48:13.445070Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/20 01:48:14 WARN UDTRegistration: Cannot register UDT for org.geotools.coverage.grid.GridCoverage2D, which is already registered.\n",
      "25/10/20 01:48:14 WARN SimpleFunctionRegistry: The function rs_union_aggr replaced a previously registered function.\n",
      "25/10/20 01:48:14 WARN UDTRegistration: Cannot register UDT for org.locationtech.jts.geom.Geometry, which is already registered.\n",
      "25/10/20 01:48:14 WARN UDTRegistration: Cannot register UDT for org.apache.sedona.common.S2Geography.Geography, which is already registered.\n",
      "25/10/20 01:48:14 WARN UDTRegistration: Cannot register UDT for org.locationtech.jts.index.SpatialIndex, which is already registered.\n",
      "25/10/20 01:48:14 WARN SimpleFunctionRegistry: The function st_union_aggr replaced a previously registered function.\n",
      "25/10/20 01:48:14 WARN SimpleFunctionRegistry: The function st_envelope_aggr replaced a previously registered function.\n",
      "25/10/20 01:48:14 WARN SimpleFunctionRegistry: The function st_intersection_aggr replaced a previously registered function.\n",
      "25/10/20 01:48:14 WARN SimpleFunctionRegistry: The function st_analyze_aggr replaced a previously registered function.\n",
      "25/10/20 01:48:14 WARN SimpleFunctionRegistry: The function rs_classify replaced a previously registered function.\n",
      "25/10/20 01:48:14 WARN SimpleFunctionRegistry: The function rs_max_confidence replaced a previously registered function.\n",
      "25/10/20 01:48:14 WARN SimpleFunctionRegistry: The function rs_segment replaced a previously registered function.\n",
      "25/10/20 01:48:14 WARN SimpleFunctionRegistry: The function rs_text_to_segments replaced a previously registered function.\n",
      "25/10/20 01:48:14 WARN SimpleFunctionRegistry: The function rs_detect_bboxes replaced a previously registered function.\n",
      "25/10/20 01:48:14 WARN SimpleFunctionRegistry: The function rs_filter_box_confidence replaced a previously registered function.\n",
      "25/10/20 01:48:14 WARN SimpleFunctionRegistry: The function rs_text_to_bboxes replaced a previously registered function.\n"
     ]
    }
   ],
   "source": [
    "from sedona.spark import *\n",
    "config = SedonaContext.builder().getOrCreate()\n",
    "sedona = SedonaContext.create(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ea6fb4c-cfce-4308-8cc0-b885d24d3da6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T01:48:15.455345Z",
     "iopub.status.busy": "2025-10-20T01:48:15.455168Z",
     "iopub.status.idle": "2025-10-20T01:48:15.458144Z",
     "shell.execute_reply": "2025-10-20T01:48:15.457475Z",
     "shell.execute_reply.started": "2025-10-20T01:48:15.455331Z"
    }
   },
   "outputs": [],
   "source": [
    "database = 'gde_silver'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25a83618-6cd9-4bd8-9788-4d84659e2903",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T01:48:16.547960Z",
     "iopub.status.busy": "2025-10-20T01:48:16.547759Z",
     "iopub.status.idle": "2025-10-20T01:48:25.051546Z",
     "shell.execute_reply": "2025-10-20T01:48:25.051072Z",
     "shell.execute_reply.started": "2025-10-20T01:48:16.547944Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Home Area Buffers\n",
    "\n",
    "sedona.sql(f'''\n",
    "create or replace table org_catalog.{database}.homes_buffers as\n",
    "select                                                                                                      \n",
    "sale_id,                                                                                                  \n",
    "ST_Buffer(geometry, 1600, true) as buffer_1mile\n",
    "from org_catalog.gde_bronze.king_co_homes \n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7bd0c942-022c-446a-9e2c-e72c921907d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T01:48:25.052220Z",
     "iopub.status.busy": "2025-10-20T01:48:25.052113Z",
     "iopub.status.idle": "2025-10-20T01:49:49.842975Z",
     "shell.execute_reply": "2025-10-20T01:49:49.842413Z",
     "shell.execute_reply.started": "2025-10-20T01:48:25.052208Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Flood Hazards Table\n",
    "\n",
    "sedona.sql(f'''\n",
    "create or replace table org_catalog.{database}.homes_demographics\n",
    "with overlaps as (\n",
    "select                                                                                                      \n",
    "a.sale_id,   \n",
    "b.total_pop,\n",
    "b.median_age,\n",
    "b.median_income,\n",
    "st_area(\n",
    "    st_intersection(a.buffer_1mile, b.geometry)\n",
    ") / st_area(b.geometry) as share                                                                                             \n",
    "from org_catalog.{database}.homes_buffers a \n",
    "join org_catalog.{database}.census_data b  \n",
    "on st_intersects(b.geometry, a.buffer_1mile)\n",
    "),\n",
    "weighted AS (\n",
    "  SELECT\n",
    "    sale_id,\n",
    "    (total_pop * share) AS pop_in_overlap,  \n",
    "    median_age,\n",
    "    median_income\n",
    "  FROM overlaps\n",
    ")\n",
    "SELECT\n",
    "  sale_id,\n",
    "  SUM(pop_in_overlap) AS est_total_pop,\n",
    "  SUM(median_income * pop_in_overlap) / NULLIF(SUM(pop_in_overlap), 0) AS pw_median_income_proxy,\n",
    "  SUM(median_age * pop_in_overlap) / NULLIF(SUM(pop_in_overlap), 0) AS pw_median_age_proxy\n",
    "FROM weighted\n",
    "GROUP BY sale_id\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1cb756eb-616b-4d7b-b0fc-4700fa67f35b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T02:33:23.878525Z",
     "iopub.status.busy": "2025-10-20T02:33:23.878318Z",
     "iopub.status.idle": "2025-10-20T03:10:11.107120Z",
     "shell.execute_reply": "2025-10-20T03:10:11.105402Z",
     "shell.execute_reply.started": "2025-10-20T02:33:23.878510Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.                 (0 + 3) / 3]\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/wherobots/lib/python3.11/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/envs/wherobots/lib/python3.11/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/envs/wherobots/lib/python3.11/socket.py\", line 718, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Zoning Percentages\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43msedona\u001b[49m\u001b[43m.\u001b[49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m'''\u001b[39;49m\n\u001b[32m      4\u001b[39m \u001b[33;43mcreate or replace table org_catalog.\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdatabase\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m.zoning_overlaps as\u001b[39;49m\n\u001b[32m      5\u001b[39m \u001b[33;43mselect                                                                                                      \u001b[39;49m\n\u001b[32m      6\u001b[39m \u001b[33;43ma.sale_id,   \u001b[39;49m\n\u001b[32m      7\u001b[39m \u001b[33;43mb.MASTER_CAT,\u001b[39;49m\n\u001b[32m      8\u001b[39m \u001b[33;43mb.SUB_CAT,\u001b[39;49m\n\u001b[32m      9\u001b[39m \u001b[33;43mst_area(\u001b[39;49m\n\u001b[32m     10\u001b[39m \u001b[33;43m    st_intersection(a.buffer_1mile, b.geometry)\u001b[39;49m\n\u001b[32m     11\u001b[39m \u001b[33;43m) / st_area(b.geometry) as share                                                                                             \u001b[39;49m\n\u001b[32m     12\u001b[39m \u001b[33;43mfrom org_catalog.\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdatabase\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m.homes_buffers a \u001b[39;49m\n\u001b[32m     13\u001b[39m \u001b[33;43mjoin org_catalog.gde_bronze.gen_land_use_bronze b  \u001b[39;49m\n\u001b[32m     14\u001b[39m \u001b[33;43mon st_intersects(b.geometry, a.buffer_1mile)\u001b[39;49m\n\u001b[32m     15\u001b[39m \u001b[33;43mwhere st_isvalid(geometry) is true\u001b[39;49m\n\u001b[32m     16\u001b[39m \u001b[33;43m'''\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/spark/python/pyspark/sql/session.py:1631\u001b[39m, in \u001b[36mSparkSession.sql\u001b[39m\u001b[34m(self, sqlQuery, args, **kwargs)\u001b[39m\n\u001b[32m   1627\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1628\u001b[39m         litArgs = \u001b[38;5;28mself\u001b[39m._jvm.PythonUtils.toArray(\n\u001b[32m   1629\u001b[39m             [_to_java_column(lit(v)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m (args \u001b[38;5;129;01mor\u001b[39;00m [])]\n\u001b[32m   1630\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1631\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jsparkSession\u001b[49m\u001b[43m.\u001b[49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlitArgs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[32m   1632\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m   1633\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) > \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/wherobots/lib/python3.11/site-packages/py4j/java_gateway.py:1321\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1314\u001b[39m args_command, temp_args = \u001b[38;5;28mself\u001b[39m._build_args(*args)\n\u001b[32m   1316\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1317\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1318\u001b[39m     args_command +\\\n\u001b[32m   1319\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m-> \u001b[39m\u001b[32m1321\u001b[39m answer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1322\u001b[39m return_value = get_return_value(\n\u001b[32m   1323\u001b[39m     answer, \u001b[38;5;28mself\u001b[39m.gateway_client, \u001b[38;5;28mself\u001b[39m.target_id, \u001b[38;5;28mself\u001b[39m.name)\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/wherobots/lib/python3.11/site-packages/py4j/java_gateway.py:1038\u001b[39m, in \u001b[36mGatewayClient.send_command\u001b[39m\u001b[34m(self, command, retry, binary)\u001b[39m\n\u001b[32m   1036\u001b[39m connection = \u001b[38;5;28mself\u001b[39m._get_connection()\n\u001b[32m   1037\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1038\u001b[39m     response = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1039\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[32m   1040\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m._create_connection_guard(connection)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/wherobots/lib/python3.11/site-packages/py4j/clientserver.py:511\u001b[39m, in \u001b[36mClientServerConnection.send_command\u001b[39m\u001b[34m(self, command)\u001b[39m\n\u001b[32m    509\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    510\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m511\u001b[39m         answer = smart_decode(\u001b[38;5;28mself\u001b[39m.stream.readline()[:-\u001b[32m1\u001b[39m])\n\u001b[32m    512\u001b[39m         logger.debug(\u001b[33m\"\u001b[39m\u001b[33mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[33m\"\u001b[39m.format(answer))\n\u001b[32m    513\u001b[39m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[32m    514\u001b[39m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/wherobots/lib/python3.11/socket.py:718\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    716\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    717\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m718\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    719\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[32m    720\u001b[39m         \u001b[38;5;28mself\u001b[39m._timeout_occurred = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/20 03:10:11 ERROR AppendDataExec: Data source write support IcebergBatchWrite(table=org_catalog.gde_silver.zoning_overlaps, format=PARQUET) is aborting.\n",
      "25/10/20 03:10:11 WARN SparkWrite: Skipping cleanup of written files\n",
      "25/10/20 03:10:11 ERROR AppendDataExec: Data source write support IcebergBatchWrite(table=org_catalog.gde_silver.zoning_overlaps, format=PARQUET) aborted.\n",
      "25/10/20 03:10:11 ERROR Utils: Aborting task\n",
      "org.apache.spark.SparkException: Job 25 cancelled as part of cancellation of all jobs\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:2731)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$doCancelAllJobs$2(DAGScheduler.scala:1114)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.scala:18)\n",
      "\tat scala.collection.mutable.HashSet$Node.foreach(HashSet.scala:435)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:361)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:1113)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3022)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:390)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2$(WriteToDataSourceV2Exec.scala:364)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.AppendDataExec.writeWithV2(WriteToDataSourceV2Exec.scala:230)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2ExistingTableWriteExec.run(WriteToDataSourceV2Exec.scala:342)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2ExistingTableWriteExec.run$(WriteToDataSourceV2Exec.scala:341)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.AppendDataExec.run(WriteToDataSourceV2Exec.scala:230)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.$anonfun$writeToTable$1(WriteToDataSourceV2Exec.scala:582)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.writeToTable(WriteToDataSourceV2Exec.scala:578)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.writeToTable$(WriteToDataSourceV2Exec.scala:572)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableAsSelectExec.writeToTable(WriteToDataSourceV2Exec.scala:186)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableAsSelectExec.run(WriteToDataSourceV2Exec.scala:221)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n",
      "\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:220)\n",
      "\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:100)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n",
      "\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)\n",
      "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "[Stage 30:>                                                         (0 + 1) / 3]\r"
     ]
    }
   ],
   "source": [
    "# Zoning Percentages\n",
    "\n",
    "sedona.sql(f'''\n",
    "create or replace table org_catalog.{database}.zoning_overlaps as\n",
    "select                                                                                                      \n",
    "a.sale_id,   \n",
    "b.MASTER_CAT,\n",
    "b.SUB_CAT,\n",
    "st_area(\n",
    "    st_intersection(a.buffer_1mile, b.geometry)\n",
    ") / st_area(b.geometry) as share                                                                                             \n",
    "from org_catalog.{database}.homes_buffers a \n",
    "join org_catalog.gde_bronze.gen_land_use_bronze b  \n",
    "on st_intersects(b.geometry, a.buffer_1mile)\n",
    "where st_isvalid(geometry) is true\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e47f9e-eae6-4449-9731-8c95cc8b0bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sedona.sql(f'''\n",
    "create or replace table org_catalog.{database}.homes_zoning_overlaps as\n",
    "SELECT\n",
    "  sale_id,\n",
    "  SUM(CASE WHEN master_cat = 'Industrial' THEN share ELSE 0.0 END) AS industrial,\n",
    "  SUM(CASE WHEN sub_cat = 'Residential (12+ Units/Acre)' THEN share ELSE 0.0 END) AS urban_residential,\n",
    "  SUM(CASE WHEN sub_cat = 'Mixed Use' THEN share ELSE 0.0 END) AS mixed_use,\n",
    "  SUM(CASE WHEN sub_cat = 'Commercial/Office' THEN share ELSE 0.0 END) AS commercial,\n",
    "  SUM(CASE WHEN master_cat = 'Active Open Space and Recreation' THEN share ELSE 0.0 END) AS recreation,\n",
    "  SUM(CASE WHEN master_cat = 'Urban Character Residential' THEN share ELSE 0.0 END) AS light_urban,\n",
    "  SUM(CASE WHEN master_cat = 'Rural Character Residential' THEN share ELSE 0.0 END) AS rural\n",
    "FROM overlaps\n",
    "GROUP BY sale_id\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc52f087-69d9-4bb7-97d5-aeb6c68661aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
