{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cacaa5d7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Overview of geospatial data, coordinate systems and geometry types.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e3100f77-fe3f-4c9a-8a0a-a4d62c723e50",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T02:28:20.837341Z",
     "iopub.status.busy": "2025-10-01T02:28:20.837117Z",
     "iopub.status.idle": "2025-10-01T02:28:20.840852Z",
     "shell.execute_reply": "2025-10-01T02:28:20.840233Z",
     "shell.execute_reply.started": "2025-10-01T02:28:20.837325Z"
    }
   },
   "outputs": [],
   "source": [
    "from sedona.spark import *\n",
    "from pyspark.sql.functions import col, when, expr\n",
    "from sedona.sql.st_functions import ST_IsValid, ST_IsValidReason, ST_MakeValid\n",
    "from pyspark.sql import DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8625818a-f65d-44c6-a5f2-a7d149c5075b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T01:50:33.045978Z",
     "iopub.status.busy": "2025-10-01T01:50:33.045621Z",
     "iopub.status.idle": "2025-10-01T01:53:07.039846Z",
     "shell.execute_reply": "2025-10-01T01:53:07.039072Z",
     "shell.execute_reply.started": "2025-10-01T01:50:33.045959Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "config = SedonaContext.builder().getOrCreate()\n",
    "sedona = SedonaContext.create(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2e45a109-8ccf-491f-9902-b8d2e185b025",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-30T18:18:29.809907Z",
     "iopub.status.busy": "2025-09-30T18:18:29.809687Z",
     "iopub.status.idle": "2025-09-30T18:18:29.812435Z",
     "shell.execute_reply": "2025-09-30T18:18:29.812062Z",
     "shell.execute_reply.started": "2025-09-30T18:18:29.809878Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e78e0763",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Intoduction to a Sedona DataFrame\n",
    "\n",
    "A Sedona DataFrame is an extension of the standard Spark DataFrame that adds native support for geospatial data types — namely Vector and Raster.\n",
    "\n",
    "- Vector data includes geometries such as points, lines, and polygons.\n",
    "- Raster represents gridded or image-based data such as satellite imagery or elevation tiles.\n",
    "\n",
    "Because these are built-in (native) data types, Sedona and Wherobots know how to handle them automatically.\n",
    "This means you can:\n",
    "- Run spatial and raster functions (like `ST_Contains`, `ST_Intersection`, `RS_Clip`, `RS_ZonalStats`) directly in your queries.\n",
    "- Combine vector and raster data in the same workflow — for example, clipping imagery to a city boundary.\n",
    "- Scale these operations easily across large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9246ee42",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Vector drivers  - Pranav\n",
    "\n",
    "# GeoJSON\n",
    "# CSV\n",
    "# Shapefile\n",
    "\n",
    "# Show others in comments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95681fa0",
   "metadata": {},
   "source": [
    "## Loading vector data into a Sedona DataFrame\n",
    "\n",
    "Now that we know what a Sedona DataFrame is, let’s see how to load vector data into one.\n",
    "\n",
    "In this section, we’ll cover:\n",
    "- GeoParquet — the most common cloud-native geospatial format\n",
    "- GeoJSON — flexible and human-readable\n",
    "- CSV — raw text with WKT/WKB geometries\n",
    "- Shapefile — the classic desktop GIS format\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591cdee0",
   "metadata": {},
   "source": [
    "### GeoParquet\n",
    "\n",
    "GeoParquet is the preferred format for storing and sharing vector data in the cloud.\n",
    "\n",
    "It extends the standard Apache Parquet format with a small block of “geo” metadata that describes:\n",
    "- which column contains the geometry,\n",
    "- the geometry type (Point, Polygon, etc.),\n",
    "- its coordinate reference system (CRS), and\n",
    "- the bounding box of each geometry column.\n",
    "\n",
    "Because GeoParquet is columnar, compressed, and splittable, it’s ideal for large-scale analytics on cloud object storage like S3.\n",
    "It also supports spatial predicate push-down — meaning Wherobots can automatically skip reading irrelevant files and row-groups when performing spatial range queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1210df95",
   "metadata": {},
   "outputs": [],
   "source": [
    "geoparquet_path = \"s3://wherobots-examples/data/nyc_buildings.parquet\"\n",
    "\n",
    "df = sedona.read.format(\"geoparquet\").load(geoparquet_path)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3851be38",
   "metadata": {},
   "source": [
    "### GeoJSON\n",
    "\n",
    "Wherobots supports reading GeoJSON files directly using the geojson data source.\n",
    "\n",
    "This reader understands most GeoJSON variations, including:\n",
    "- Standard Feature and FeatureCollection objects\n",
    "- SpatioTemporal Asset Catalog (STAC) files\n",
    "- GeoJSON files that span multiple lines for readability\n",
    "\n",
    "When loaded, Wherobots automatically parses the geometry field into its internal Geometry type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8039e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "geojson_multi_path = \"s3://wherobots-examples/data/noaa/storms/\"\n",
    "\n",
    "df = sedona.read.format(\"geojson\")\\\n",
    "        .option(\"multiLine\", \"true\")\\\n",
    "        .load(geojson_multi_path)\\\n",
    "        .selectExpr(\"explode(features) as features\")\\\n",
    "        .select(\"features.*\")\n",
    "\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8613e6",
   "metadata": {},
   "source": [
    "### Shapefile\n",
    "\n",
    "The Shapefile format has been around since the early days of GIS and is still used widely in desktop and government datasets.\n",
    "\n",
    "Wherobots can load Shapefiles directly into a Sedona DataFrame using the shapefile data source.\n",
    "This works whether you point to a single .shp file or to a directory containing multiple shapefiles.\n",
    "\n",
    "Wherobots automatically reads the related files (.dbf, .shx, etc.) and converts the geometry column into a native Geometry type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd16fedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "shp_path = \"s3://wherobots-examples/data/austin_boundaries/\"\n",
    "\n",
    "df = sedona.read.format(\"shapefile\").load(shp_path)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db71f6d",
   "metadata": {},
   "source": [
    "When the input path is a directory, all shapefiles directly inside that directory will be loaded.\n",
    "To include shapefiles in subdirectories, set recursiveFileLookup to true:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8d36bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sedona.read.format(\"shapefile\") \\\n",
    "    .option(\"recursiveFileLookup\", \"true\") \\\n",
    "    .load(\"s3://wherobots-examples/data/examples/Global_Landslide_Catalog/\")\n",
    "\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9e1a44",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Raster - Pranav\n",
    "\n",
    "# Raster\n",
    "\n",
    "# Show others"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09dcec5",
   "metadata": {},
   "source": [
    "## Loading raster data into a Sedona DataFrame\n",
    "\n",
    "Wherobots can load rasters as native raster columns, allowing you to tile, clip, resample, and compute statistics directly in Spark - just like you would with tabular data.\n",
    "\n",
    "We’ll cover:\n",
    "- Reading GeoTIFFs (COGs recommended)\n",
    "- Reading from STAC APIs and collections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3fac4d2",
   "metadata": {},
   "source": [
    "### GeoTIFFs\n",
    "\n",
    "GeoTIFFs are raster image files that store both pixel values and geospatial metadata (like coordinate reference systems).\n",
    "A Cloud-Optimized GeoTIFF (COG) is a GeoTIFF structured for fast, partial reads in cloud storage — ideal for distributed systems like Wherobots.\n",
    "\n",
    "Wherobots' raster reader can load these directly into a Sedona DataFrame.\n",
    "Each raster becomes one or more tiles, stored in a column of type raster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053b2d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "geotiff_path = \"s3://wherobots-examples/data/ghs_population/GHS_POP_E1975_GLOBE_R2023A_4326_3ss_V1_0.tif\"\n",
    "\n",
    "df = sedona.read.format(\"raster\").load(geotiff_path)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35cb060b",
   "metadata": {},
   "source": [
    "Each row represents a raster tile, and by default the raster reader automatically:\n",
    "- Splits large rasters into tiles.\n",
    "- Adds x and y columns to indicate each tile’s position.\n",
    "- Reads file-level metadata (CRS, extent, etc.) internally.\n",
    "\n",
    "To fine-tune the tiling behavior, you can specify options:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5579d419",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sedona.read.format(\"raster\") \\\n",
    "    .option(\"tileWidth\", \"512\") \\\n",
    "    .option(\"tileHeight\", \"512\") \\\n",
    "    .option(\"retile\", \"true\") \\\n",
    "    .load(geotiff_path)\n",
    "\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef47856",
   "metadata": {},
   "source": [
    "> Tip - Use Cloud-Optimized GeoTIFFs (COGs) when possible — they’re optimized for partial reads in cloud storage, which makes distributed processing far more efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e67be62",
   "metadata": {},
   "source": [
    "### SpatioTemporal Asset Catalog (STAC)\n",
    "\n",
    "A SpatioTemporal Asset Catalog (STAC) is a standard for describing and organizing geospatial assets — such as satellite imagery, aerial photos, and elevation data — across space and time.\n",
    "\n",
    "Wherobots includes a built-in STAC Reader, which allows you to load STAC items and collections directly into a Sedona DataFrame. This gives you seamless access to large imagery archives hosted on cloud platforms like AWS, Planetary Computer, or Element84 — all without leaving your Spark environment.\n",
    "\n",
    "The STAC Reader supports:\n",
    "- Direct integration with HTTP, HTTPS, S3, or local STAC JSON sources.\n",
    "- Unified geospatial analysis, so you can join imagery metadata with vector or raster datasets inside Spark.\n",
    "- Spatial and temporal filter pushdown, meaning filters like `ST_Intersects` or datetime BETWEEN are pushed down to the STAC API, minimizing data transfer and improving query performance.\n",
    "- Flexible configuration options for partitioning, request limits, and parallel loading — making it scalable for both exploration and production workflows.\n",
    "\n",
    "You can connect to STAC sources via an HTTPS endpoint, an S3 path, or a local JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72710a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "stac_df = sedona.read.format(\"stac\").load(\n",
    "    \"https://earth-search.aws.element84.com/v1/collections/sentinel-2-pre-c1-l2a\"\n",
    ")\n",
    "\n",
    "stac_df.printSchema()\n",
    "stac_df.select(\"id\", \"datetime\", \"geometry\", \"collection\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2620aa",
   "metadata": {},
   "source": [
    "You can control how many items to load, how requests are batched, and how partitions are distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2618b0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sedona.read.format(\"stac\") \\\n",
    "            .option(\"itemsLimitMax\", \"1000\")\\\n",
    "            .option(\"itemsLimitPerRequest\", \"50\")\\\n",
    "            .option(\"itemsLoadProcessReportThreshold\", \"500000\")\\\n",
    "            .load(\"https://earth-search.aws.element84.com/v1/collections/sentinel-2-pre-c1-l2a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45d41f0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Intro to Iceberg - Pranav"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9bf07e9",
   "metadata": {},
   "source": [
    "## Introduction to Managing Spatial Tables with Iceberg\n",
    "\n",
    "Wherobots extends Apache Iceberg — a modern open table format — to natively support spatial data.\n",
    "This allows you to manage vector and raster datasets just like any other analytical table, with the same reliability, scalability, and query optimization benefits.\n",
    "\n",
    "For a data engineer, this means you can use familiar tools (SQL, Spark, Sedona) while gaining spatial awareness at the table level — no need for custom file handling or geospatial indexing setups.\n",
    "\n",
    "### Why Iceberg Matters for Spatial Data\n",
    "\n",
    "Iceberg brings all the essentials of modern data lake management — schema evolution, ACID transactions, partition pruning, and time travel.\n",
    "Wherobots builds on this foundation to add spatial intelligence directly into the table layer.\n",
    "\n",
    "1. Native Spatial Columns\n",
    "\n",
    "    Geometry and raster columns are first-class types — not just blobs or strings.\n",
    "\n",
    "    This means:\n",
    "    - You can save Sedona DataFrames with geometry or raster columns directly to Iceberg tables.\n",
    "    - Query them with familiar functions like `ST_Intersects`, `ST_Within`, or `RS_Clip`.\n",
    "    - Work with both in-database rasters (stored in memory) and out-of-database rasters (linked to GeoTIFFs or COGs on S3).\n",
    "\n",
    "2. Spatial Statistics and Pruning\n",
    "\n",
    "    Each Iceberg data file stores spatial metadata — including minimum bounding rectangles (MBRs) for geometry and raster columns.\n",
    "\n",
    "    This allows the query engine to:\n",
    "    - Skip irrelevant files that fall outside your spatial filter (spatial pruning).\n",
    "    - Push down bounding-box filters to the scan layer, reducing data read from storage.\n",
    "    - Speed up spatial joins by comparing file-level extents before loading data.\n",
    "\n",
    "3. Spatial Partitioning and Organization\n",
    "\n",
    "    Spatial transformations (like tiling, grid partitioning, or bounding-box bucketing) can be used to organize data.\n",
    "    This helps co-locate nearby geometries and tiles, reducing shuffle and improving performance in spatial joins or aggregations.\n",
    "\n",
    "4. Query Optimization and Pushdown\n",
    "\n",
    "    WherobotsDB uses Iceberg’s metadata to push down both spatial and temporal filters:\n",
    "    - Spatial filters (`ST_Intersects`, `ST_Within`) are evaluated at the metadata level.\n",
    "    - Raster metadata and specific bands can be selectively read (projection pushdown)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fba1939",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Transforming data with non-native readers - in slides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94d5674",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Wherobots Fundamentals - Constructing Geometries - Furqaan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7454b482",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Wherobots Fundamentals - Spatial predicates  - Furqaan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34648efc",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Wherobots Fundamentals - Range joins - Furqaan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62fbfc11",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# create and manage Havasu (Iceberg) tables for vector and raster data  - Furqaan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f13332e",
   "metadata": {},
   "source": [
    "# Data validity checks\n",
    "\n",
    "Two of the most common issues with geospatial data include managing projections or Coordinate Reference Systems (CRS) and ensuring geometries are valid.\n",
    "\n",
    "- A geometry is invalid if it violates spatial rules like self-intersections, unclosed rings, misaligned holes, or overlapping parts—making it topologically incorrect.\n",
    "- Spatial files generally contain a Coordinate Reference System or CRS that is defined by a Spatial Reference ID or SRID. This tells us how the data is projected from the round spheroid of the earth onto a flat surface.\n",
    "\n",
    "To fix these issues and ensure our data is valid and in the correct format we use two approaches:\n",
    "\n",
    "1. Check the geometries for any invalidities, and if there are attempt to fix them using `ST_IsValid`, `ST_IsValidDetail`, and `ST_MakeValid`\n",
    "2. Remove or log out any geometries that cannot be fixed\n",
    "3. Standardize our geometries in a single CRS, in this case [EPSG:4326](https://epsg.io/4326) which renders in a coordinate reference system"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54fb11ae",
   "metadata": {},
   "source": [
    "## Validating geometries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf2ec1a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Data validity checks - Matt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b016f7b1-3efc-40ca-8eb4-502ae60f1d33",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Transforming CRS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384c5eeb",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Handling and transforming CRS - Matt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d489c6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Dataset loading - aka load all datasets to tables - Matt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165d277b",
   "metadata": {},
   "source": [
    "# Loading datasets into WherobotsDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b71f7ea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T01:55:45.137550Z",
     "iopub.status.busy": "2025-10-01T01:55:45.137306Z",
     "iopub.status.idle": "2025-10-01T01:55:45.140655Z",
     "shell.execute_reply": "2025-10-01T01:55:45.140155Z",
     "shell.execute_reply.started": "2025-10-01T01:55:45.137530Z"
    },
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "prefix = 's3://wherobots-examples/gdea-course-data/raw-data/'\n",
    "database = 'gde_bronze'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf1c469e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T01:55:46.380833Z",
     "iopub.status.busy": "2025-10-01T01:55:46.380611Z",
     "iopub.status.idle": "2025-10-01T01:55:50.385842Z",
     "shell.execute_reply": "2025-10-01T01:55:50.385430Z",
     "shell.execute_reply.started": "2025-10-01T01:55:46.380816Z"
    },
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sedona.sql(f'CREATE DATABASE IF NOT EXISTS wherobots.{database}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "958cc513-5569-413a-9ede-3f5c0e586a4f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T01:55:52.056645Z",
     "iopub.status.busy": "2025-10-01T01:55:52.056434Z",
     "iopub.status.idle": "2025-10-01T01:55:52.063339Z",
     "shell.execute_reply": "2025-10-01T01:55:52.062895Z",
     "shell.execute_reply.started": "2025-10-01T01:55:52.056629Z"
    }
   },
   "outputs": [],
   "source": [
    "def check_invalid_geometries(df: DataFrame, geom_col: str = \"geom\", reason_col: str = \"why_invalid\") -> int:\n",
    "    df_with_reason = df.withColumn(reason_col, ST_IsValidReason(col(geom_col)))\n",
    "    # cache to avoid recomputation if you inspect reasons later\n",
    "    df_with_reason.cache()\n",
    "    invalid_count = df_with_reason.filter(~ST_IsValid(col(geom_col))).count()\n",
    "    print(f\"✅ Checked geometries — found {invalid_count} invalid geometries.\")\n",
    "    return invalid_count\n",
    "\n",
    "def fix_invalid_geometries(df: DataFrame, invalid_count: int, geom_col: str = \"geom\") -> DataFrame:\n",
    "    if invalid_count > 1:\n",
    "        print(f\"🔧 Attempting to fix {invalid_count} invalid geometries...\")\n",
    "        return df.withColumn(\n",
    "            geom_col,\n",
    "            when(~ST_IsValid(col(geom_col)), ST_MakeValid(col(geom_col))).otherwise(col(geom_col))\n",
    "        )\n",
    "    else:\n",
    "        print(\"⚡ Only one invalid geometry (or none). Skipping automated fix.\")\n",
    "        return df\n",
    "\n",
    "# --- driver program ---\n",
    "def process_geometries(\n",
    "    df: DataFrame,\n",
    "    geom_col: str = \"geom\",\n",
    "    attempt_fix: bool = True,\n",
    "    split_on_fail: bool = True\n",
    "):\n",
    "    \"\"\"\n",
    "    Runs validity check -> optional repair -> optional split.\n",
    "    Returns either:\n",
    "      - {\"df\": corrected_df}  when all geometries valid after repair (or none invalid)\n",
    "      - {\"valid_df\": ..., \"invalid_df\": ...} when some invalid remain and split_on_fail=True\n",
    "    \"\"\"\n",
    "    # 1) Initial check\n",
    "    invalid_count = check_invalid_geometries(df, geom_col=geom_col)\n",
    "\n",
    "    if invalid_count == 0:\n",
    "        print(\"✅ All geometries are valid.\")\n",
    "        return {\"df\": df}  # nothing to do\n",
    "\n",
    "    # 2) Attempt repair (only changes rows that are invalid per your earlier contract)\n",
    "    if attempt_fix:\n",
    "        df_fixed = fix_invalid_geometries(df, invalid_count, geom_col=geom_col)\n",
    "        remaining_invalid_count = df_fixed.filter(~ST_IsValid(col(geom_col))).count()\n",
    "        print(f\"🔎 After fixing, {remaining_invalid_count} invalid geometries remain.\")\n",
    "    \n",
    "        if remaining_invalid_count == 0:\n",
    "            print(\"✅ All geometries are valid after fixing.\")\n",
    "            return {\"df\": df_fixed}\n",
    "        elif split_on_fail:\n",
    "            print(\"⚠️ Some invalid geometries remain — splitting dataset.\")\n",
    "            valid_df = df_fixed.filter(ST_IsValid(col(geom_col)))\n",
    "            invalid_df = df_fixed.filter(~ST_IsValid(col(geom_col)))\n",
    "            print(f\"✅ Split complete: {valid_df.count()} valid / {invalid_df.count()} invalid.\")\n",
    "            return {\"valid_df\": valid_df, \"invalid_df\": invalid_df}\n",
    "        else:\n",
    "            print(\"⚠️ Some invalid geometries remain, returning best-effort fixed DataFrame.\")\n",
    "            return {\"df\": df_fixed}\n",
    "    \n",
    "    # If no fix attempt, just split if requested\n",
    "    if split_on_fail:\n",
    "        print(\"⚠️ Skipping fix — splitting into valid and invalid.\")\n",
    "        valid_df = df.filter(ST_IsValid(col(geom_col)))\n",
    "        invalid_df = df.filter(~ST_IsValid(col(geom_col)))\n",
    "        print(f\"✅ Split complete: {valid_df.count()} valid / {invalid_df.count()} invalid.\")\n",
    "        return {\"valid_df\": valid_df, \"invalid_df\": invalid_df}\n",
    "    \n",
    "    print(\"⚠️ Invalid geometries found but no fix or split requested. Returning original DataFrame.\")\n",
    "    return {\"df\": df}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e34a2ada-50cd-4bc8-aeb9-4ffc5aa5ef0e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T01:55:53.660177Z",
     "iopub.status.busy": "2025-10-01T01:55:53.659965Z",
     "iopub.status.idle": "2025-10-01T01:55:55.891021Z",
     "shell.execute_reply": "2025-10-01T01:55:55.890129Z",
     "shell.execute_reply.started": "2025-10-01T01:55:53.660161Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# FEMA Flood Hazard Areas\n",
    "fld_hazard_area = sedona.read.format('shapefile').load(f'{prefix}' + '53033C_20250330/S_FLD_HAZ_AR.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fc0531c3-b464-426a-b6c2-d4cc2243f0bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-30T18:43:28.218185Z",
     "iopub.status.busy": "2025-09-30T18:43:28.217980Z",
     "iopub.status.idle": "2025-09-30T18:43:32.280299Z",
     "shell.execute_reply": "2025-09-30T18:43:32.279877Z",
     "shell.execute_reply.started": "2025-09-30T18:43:28.218170Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/30 18:43:28 WARN CacheManager: Asked to cache already cached data.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Checked geometries — found 15 invalid geometries.\n",
      "🔧 Attempting to fix 15 invalid geometries...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 37:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔎 After fixing, 0 invalid geometries remain.\n",
      "✅ All geometries are valid after fixing.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "result = process_geometries(fld_hazard_area, geom_col=\"geometry\", attempt_fix=True, split_on_fail=True)\n",
    "\n",
    "if \"df\" in result:\n",
    "    df_final = result[\"df\"]  # all valid (either already valid or successfully repaired)\n",
    "else:\n",
    "    valid_df = result[\"valid_df\"]\n",
    "    invalid_df = result[\"invalid_df\"]\n",
    "    # handle invalids (e.g., export for manual review)(fld_hazard_area, 'geometry')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "654d631f-9d25-47ee-be41-b2480ef9018b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-30T18:43:35.309951Z",
     "iopub.status.busy": "2025-09-30T18:43:35.309729Z",
     "iopub.status.idle": "2025-09-30T18:43:37.712061Z",
     "shell.execute_reply": "2025-09-30T18:43:37.711481Z",
     "shell.execute_reply.started": "2025-09-30T18:43:35.309935Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_final.writeTo(f\"wherobots.{database}.fema_flood_zones_bronze\").createOrReplace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "80c6b264-da6b-4a4e-bc4f-4e1ad86896da",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-30T18:43:37.712886Z",
     "iopub.status.busy": "2025-09-30T18:43:37.712755Z",
     "iopub.status.idle": "2025-09-30T18:43:38.893841Z",
     "shell.execute_reply": "2025-09-30T18:43:38.893503Z",
     "shell.execute_reply.started": "2025-09-30T18:43:37.712870Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# King County Generalized Land Use Data\n",
    "gen_land_use = sedona.read.format('shapefile').load(f'{prefix}' + 'General_Land_Use_Final_Dataset/General_Land_Use_Final_Dataset.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "149331f3-509d-4c99-bc02-60320289d3ca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-30T18:43:38.894456Z",
     "iopub.status.busy": "2025-09-30T18:43:38.894338Z",
     "iopub.status.idle": "2025-09-30T18:44:48.020339Z",
     "shell.execute_reply": "2025-09-30T18:44:48.019878Z",
     "shell.execute_reply.started": "2025-09-30T18:43:38.894444Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Checked geometries — found 2987 invalid geometries.\n",
      "🔧 Attempting to fix 2987 invalid geometries...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 46:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔎 After fixing, 0 invalid geometries remain.\n",
      "✅ All geometries are valid after fixing.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "result = process_geometries(gen_land_use, geom_col=\"geometry\", attempt_fix=True, split_on_fail=True)\n",
    "\n",
    "if \"df\" in result:\n",
    "    df_final = result[\"df\"]  # all valid (either already valid or successfully repaired)\n",
    "else:\n",
    "    valid_df = result[\"valid_df\"]\n",
    "    invalid_df = result[\"invalid_df\"]\n",
    "    # handle invalids (e.g., export for manual review)(fld_hazard_area, 'geometry')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "19728834-c881-41e2-8449-8737f47343cb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-30T18:44:48.021152Z",
     "iopub.status.busy": "2025-09-30T18:44:48.021004Z",
     "iopub.status.idle": "2025-09-30T18:45:31.819161Z",
     "shell.execute_reply": "2025-09-30T18:45:31.818717Z",
     "shell.execute_reply.started": "2025-09-30T18:44:48.021140Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "valid_df.writeTo(f\"wherobots.{database}.gen_land_use_bronze\").createOrReplace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0bd61b53-a501-4144-a796-cf3d8859163a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-30T18:50:41.225455Z",
     "iopub.status.busy": "2025-09-30T18:50:41.225255Z",
     "iopub.status.idle": "2025-09-30T18:50:42.449358Z",
     "shell.execute_reply": "2025-09-30T18:50:42.448917Z",
     "shell.execute_reply.started": "2025-09-30T18:50:41.225439Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# King County Sherrif Patrol Districts\n",
    "sherrif_districts = sedona.read.format('shapefile').load(f'{prefix}' + 'King_County_Sheriff_Patrol_Districts___patrol_districts_area/King_County_Sheriff_Patrol_Districts___patrol_districts_area.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "542baeea-ee1b-4386-bd8b-6556666dbe62",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-30T18:51:21.944727Z",
     "iopub.status.busy": "2025-09-30T18:51:21.944526Z",
     "iopub.status.idle": "2025-09-30T18:51:29.889185Z",
     "shell.execute_reply": "2025-09-30T18:51:29.888755Z",
     "shell.execute_reply.started": "2025-09-30T18:51:21.944713Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 51:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Checked geometries — found 0 invalid geometries.\n",
      "✅ All geometries are valid.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "result = process_geometries(sherrif_districts, geom_col=\"geometry\", attempt_fix=True, split_on_fail=True)\n",
    "\n",
    "if \"df\" in result:\n",
    "    df_final = result[\"df\"]  # all valid (either already valid or successfully repaired)\n",
    "else:\n",
    "    valid_df = result[\"valid_df\"]\n",
    "    invalid_df = result[\"invalid_df\"]\n",
    "    # handle invalids (e.g., export for manual review)(fld_hazard_area, 'geometry')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6ab6769b-9897-4635-ac15-450d10cf6b2f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-30T18:51:47.848794Z",
     "iopub.status.busy": "2025-09-30T18:51:47.848561Z",
     "iopub.status.idle": "2025-09-30T18:51:57.211178Z",
     "shell.execute_reply": "2025-09-30T18:51:57.210175Z",
     "shell.execute_reply.started": "2025-09-30T18:51:47.848779Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_final.writeTo(f\"wherobots.{database}.sherrif_districts_bronze\").createOrReplace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "38a27b67-613c-4793-99c2-a0d8c82efa23",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-30T18:51:57.211971Z",
     "iopub.status.busy": "2025-09-30T18:51:57.211859Z",
     "iopub.status.idle": "2025-09-30T18:51:57.772699Z",
     "shell.execute_reply": "2025-09-30T18:51:57.771921Z",
     "shell.execute_reply.started": "2025-09-30T18:51:57.211959Z"
    }
   },
   "outputs": [],
   "source": [
    "offense_reports = sedona.read.format('csv').load(f'{prefix}' + 'KCSO_Offense_Reports__2020_to_Present_20250923.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f367d35a-e5e0-4a94-a5ea-4542a2030e93",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-30T18:52:03.463579Z",
     "iopub.status.busy": "2025-09-30T18:52:03.463372Z",
     "iopub.status.idle": "2025-09-30T18:52:06.601453Z",
     "shell.execute_reply": "2025-09-30T18:52:06.600785Z",
     "shell.execute_reply.started": "2025-09-30T18:52:03.463565Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "offense_reports.writeTo(f\"wherobots.{database}.offense_reports_bronze\").createOrReplace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a01f89e6-6075-4a93-9703-4cc040292382",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-30T18:53:16.841249Z",
     "iopub.status.busy": "2025-09-30T18:53:16.841042Z",
     "iopub.status.idle": "2025-09-30T18:53:17.291763Z",
     "shell.execute_reply": "2025-09-30T18:53:17.291106Z",
     "shell.execute_reply.started": "2025-09-30T18:53:16.841235Z"
    }
   },
   "outputs": [],
   "source": [
    "# King County Bike Lanes\n",
    "bike_lanes = sedona.read.format('shapefile').load(f'{prefix}' + 'Metro_Transportation_Network_(TNET)_in_King_County_for_Bicycle_Mode___trans_network_bike_line/Metro_Transportation_Network_(TNET)_in_King_County_for_Bicycle_Mode___trans_network_bike_line.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "09b6ddae-a8d2-4e7b-9d1a-f2467088f197",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-30T18:53:22.718728Z",
     "iopub.status.busy": "2025-09-30T18:53:22.718497Z",
     "iopub.status.idle": "2025-09-30T18:53:34.581955Z",
     "shell.execute_reply": "2025-09-30T18:53:34.581521Z",
     "shell.execute_reply.started": "2025-09-30T18:53:22.718711Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 59:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Checked geometries — found 0 invalid geometries.\n",
      "✅ All geometries are valid.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "result = process_geometries(bike_lanes, geom_col=\"geometry\", attempt_fix=True, split_on_fail=True)\n",
    "\n",
    "if \"df\" in result:\n",
    "    df_final = result[\"df\"]  # all valid (either already valid or successfully repaired)\n",
    "else:\n",
    "    valid_df = result[\"valid_df\"]\n",
    "    invalid_df = result[\"invalid_df\"]\n",
    "    # handle invalids (e.g., export for manual review)(fld_hazard_area, 'geometry')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ac773691-ecaa-46f4-8b01-8407b9059015",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-30T18:53:41.887141Z",
     "iopub.status.busy": "2025-09-30T18:53:41.886938Z",
     "iopub.status.idle": "2025-09-30T18:53:52.253464Z",
     "shell.execute_reply": "2025-09-30T18:53:52.253064Z",
     "shell.execute_reply.started": "2025-09-30T18:53:41.887125Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_final.writeTo(f\"wherobots.{database}.bike_lanes_bronze\").createOrReplace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5929ef8b-0ff9-4178-8d12-f529297945de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-30T18:56:46.162425Z",
     "iopub.status.busy": "2025-09-30T18:56:46.162221Z",
     "iopub.status.idle": "2025-09-30T18:56:46.677148Z",
     "shell.execute_reply": "2025-09-30T18:56:46.676716Z",
     "shell.execute_reply.started": "2025-09-30T18:56:46.162410Z"
    }
   },
   "outputs": [],
   "source": [
    "# FEMA National Risk Index\n",
    "fema_nri = sedona.read.format('shapefile').load(f'{prefix}' + 'NRI_Shapefile_CensusTracts/NRI_Shapefile_CensusTracts.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6b6568ac-091f-4675-a7f7-92e40da1f2b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-30T19:01:47.267173Z",
     "iopub.status.busy": "2025-09-30T19:01:47.266954Z",
     "iopub.status.idle": "2025-09-30T19:02:14.940079Z",
     "shell.execute_reply": "2025-09-30T19:02:14.939623Z",
     "shell.execute_reply.started": "2025-09-30T19:01:47.267158Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/30 19:01:47 WARN CacheManager: Asked to cache already cached data.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Checked geometries — found 82 invalid geometries.\n",
      "🔧 Attempting to fix 82 invalid geometries...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 75:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔎 After fixing, 0 invalid geometries remain.\n",
      "✅ All geometries are valid after fixing.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "result = process_geometries(fema_nri, geom_col=\"geometry\", attempt_fix=True, split_on_fail=True)\n",
    "\n",
    "if \"df\" in result:\n",
    "    df_final = result[\"df\"]  # all valid (either already valid or successfully repaired)\n",
    "else:\n",
    "    valid_df = result[\"valid_df\"]\n",
    "    invalid_df = result[\"invalid_df\"]\n",
    "    # handle invalids (e.g., export for manual review)(fld_hazard_area, 'geometry')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "35e82d32-ce21-42b3-8589-48cc16998186",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-30T19:02:14.940892Z",
     "iopub.status.busy": "2025-09-30T19:02:14.940774Z",
     "iopub.status.idle": "2025-09-30T19:03:20.925754Z",
     "shell.execute_reply": "2025-09-30T19:03:20.925227Z",
     "shell.execute_reply.started": "2025-09-30T19:02:14.940880Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/30 19:19:03 WARN ExecutorPodsWatchSnapshotSource: Kubernetes client has been closed.\n"
     ]
    }
   ],
   "source": [
    "df_final.writeTo(f\"wherobots.{database}.fema_nri_bronze\").createOrReplace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab34dd50-0527-49b7-a969-fcea5859b708",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T01:56:06.329053Z",
     "iopub.status.busy": "2025-10-01T01:56:06.328820Z",
     "iopub.status.idle": "2025-10-01T01:56:07.664491Z",
     "shell.execute_reply": "2025-10-01T01:56:07.663892Z",
     "shell.execute_reply.started": "2025-10-01T01:56:06.329036Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# King County School Sites\n",
    "school_sites = sedona.read.format('shapefile').load(f'{prefix}' + 'School_Sites_in_King_County___schsite_point/School_Sites_in_King_County___schsite_point.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "350a2a7e-dcb4-4bee-aa3c-e01f4a9b404c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T01:56:15.672606Z",
     "iopub.status.busy": "2025-10-01T01:56:15.672388Z",
     "iopub.status.idle": "2025-10-01T01:56:24.296159Z",
     "shell.execute_reply": "2025-10-01T01:56:24.295753Z",
     "shell.execute_reply.started": "2025-10-01T01:56:15.672591Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Checked geometries — found 0 invalid geometries.\n",
      "✅ All geometries are valid.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "result = process_geometries(school_sites, geom_col=\"geometry\", attempt_fix=True, split_on_fail=True)\n",
    "\n",
    "if \"df\" in result:\n",
    "    df_final = result[\"df\"]  # all valid (either already valid or successfully repaired)\n",
    "else:\n",
    "    valid_df = result[\"valid_df\"]\n",
    "    invalid_df = result[\"invalid_df\"]\n",
    "    # handle invalids (e.g., export for manual review)(fld_hazard_area, 'geometry')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a913b76-ec1c-435b-b1e6-694d15ca157f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T01:56:27.584765Z",
     "iopub.status.busy": "2025-10-01T01:56:27.584545Z",
     "iopub.status.idle": "2025-10-01T01:56:37.144022Z",
     "shell.execute_reply": "2025-10-01T01:56:37.143565Z",
     "shell.execute_reply.started": "2025-10-01T01:56:27.584749Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_final.writeTo(f\"wherobots.{database}.school_sites_bronze\").createOrReplace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "517cf454-e658-48a1-955f-56092ad3bf7c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T02:01:21.104209Z",
     "iopub.status.busy": "2025-10-01T02:01:21.103988Z",
     "iopub.status.idle": "2025-10-01T02:01:21.555254Z",
     "shell.execute_reply": "2025-10-01T02:01:21.554572Z",
     "shell.execute_reply.started": "2025-10-01T02:01:21.104194Z"
    }
   },
   "outputs": [],
   "source": [
    "# Schools Report Card\n",
    "report_card = sedona.read. \\\n",
    "    format('csv'). \\\n",
    "    load(f'{prefix}' + 'Report_Card_Growth_for_2024-25_20250923.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2a37eaa3-fb75-433d-86ec-265d3f6273e3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T02:01:21.556105Z",
     "iopub.status.busy": "2025-10-01T02:01:21.555983Z",
     "iopub.status.idle": "2025-10-01T02:01:23.640309Z",
     "shell.execute_reply": "2025-10-01T02:01:23.639631Z",
     "shell.execute_reply.started": "2025-10-01T02:01:21.556092Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "report_card.writeTo(f\"wherobots.{database}.report_card_bronze\").createOrReplace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0907b912-285b-409d-9373-53bf49920b99",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T02:01:26.675366Z",
     "iopub.status.busy": "2025-10-01T02:01:26.675163Z",
     "iopub.status.idle": "2025-10-01T02:01:27.062354Z",
     "shell.execute_reply": "2025-10-01T02:01:27.061843Z",
     "shell.execute_reply.started": "2025-10-01T02:01:26.675351Z"
    }
   },
   "outputs": [],
   "source": [
    "# Seismic Hazards\n",
    "seismic_hazards = sedona.read. \\\n",
    "    format('shapefile'). \\\n",
    "    load(f'{prefix}' + 'Seismic_Hazards___seism_area/Seismic_Hazards___seism_area.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "faea4cf1-016f-4e15-82fb-9d5872a384f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T02:01:27.157905Z",
     "iopub.status.busy": "2025-10-01T02:01:27.157686Z",
     "iopub.status.idle": "2025-10-01T02:01:27.267155Z",
     "shell.execute_reply": "2025-10-01T02:01:27.266778Z",
     "shell.execute_reply.started": "2025-10-01T02:01:27.157889Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Checked geometries — found 0 invalid geometries.\n",
      "✅ All geometries are valid.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/01 02:01:27 WARN CacheManager: Asked to cache already cached data.\n"
     ]
    }
   ],
   "source": [
    "result = process_geometries(seismic_hazards, geom_col=\"geometry\", attempt_fix=True, split_on_fail=True)\n",
    "\n",
    "if \"df\" in result:\n",
    "    df_final = result[\"df\"]  # all valid (either already valid or successfully repaired)\n",
    "else:\n",
    "    valid_df = result[\"valid_df\"]\n",
    "    invalid_df = result[\"invalid_df\"]\n",
    "    # handle invalids (e.g., export for manual review)(fld_hazard_area, 'geometry')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "93d2d673-ce35-48bb-82d1-36d8c432a392",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T02:01:32.425360Z",
     "iopub.status.busy": "2025-10-01T02:01:32.425141Z",
     "iopub.status.idle": "2025-10-01T02:01:39.065892Z",
     "shell.execute_reply": "2025-10-01T02:01:39.065461Z",
     "shell.execute_reply.started": "2025-10-01T02:01:32.425345Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_final.writeTo(f\"wherobots.{database}.seismic_hazards_bronze\").createOrReplace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "848c1509-27a0-42ec-b8cc-5b6b58c3287e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T02:01:54.001000Z",
     "iopub.status.busy": "2025-10-01T02:01:54.000781Z",
     "iopub.status.idle": "2025-10-01T02:01:54.370608Z",
     "shell.execute_reply": "2025-10-01T02:01:54.370015Z",
     "shell.execute_reply.started": "2025-10-01T02:01:54.000985Z"
    }
   },
   "outputs": [],
   "source": [
    "# Census Block Groups\n",
    "block_groups = sedona.read. \\\n",
    "    format('shapefile'). \\\n",
    "    load(f'{prefix}' + 'tl_2024_53_bg/tl_2024_53_bg.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9f47e586-1703-4dc1-abc8-7ac226fd1f4e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T02:01:54.828552Z",
     "iopub.status.busy": "2025-10-01T02:01:54.828333Z",
     "iopub.status.idle": "2025-10-01T02:01:55.155472Z",
     "shell.execute_reply": "2025-10-01T02:01:55.154911Z",
     "shell.execute_reply.started": "2025-10-01T02:01:54.828536Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/01 02:01:54 WARN CacheManager: Asked to cache already cached data.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Checked geometries — found 0 invalid geometries.\n",
      "✅ All geometries are valid.\n"
     ]
    }
   ],
   "source": [
    "result = process_geometries(block_groups, geom_col=\"geometry\", attempt_fix=True, split_on_fail=True)\n",
    "\n",
    "if \"df\" in result:\n",
    "    df_final = result[\"df\"]  # all valid (either already valid or successfully repaired)\n",
    "else:\n",
    "    valid_df = result[\"valid_df\"]\n",
    "    invalid_df = result[\"invalid_df\"]\n",
    "    # handle invalids (e.g., export for manual review)(fld_hazard_area, 'geometry')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "218d1da0-4276-45c0-8838-ec202257210a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T02:01:57.831933Z",
     "iopub.status.busy": "2025-10-01T02:01:57.831713Z",
     "iopub.status.idle": "2025-10-01T02:01:59.943150Z",
     "shell.execute_reply": "2025-10-01T02:01:59.942683Z",
     "shell.execute_reply.started": "2025-10-01T02:01:57.831917Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_final.writeTo(f\"wherobots.{database}.block_groups_bronze\").createOrReplace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "103d5f5f-ddf8-47f4-a5f1-5fb77496bac2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T02:04:26.679350Z",
     "iopub.status.busy": "2025-10-01T02:04:26.679142Z",
     "iopub.status.idle": "2025-10-01T02:04:30.963106Z",
     "shell.execute_reply": "2025-10-01T02:04:30.962334Z",
     "shell.execute_reply.started": "2025-10-01T02:04:26.679334Z"
    }
   },
   "outputs": [],
   "source": [
    "# Census CSVs\n",
    "median_age = sedona.read. \\\n",
    "    format('csv'). \\\n",
    "    load(f'{prefix}' + 'ACSDT5Y2023.B01002_2025-09-19T105233/ACSDT5Y2023.B01002-Data.csv')\n",
    "\n",
    "median_age.writeTo(f\"wherobots.{database}.median_age_bronze\").createOrReplace()\n",
    "\n",
    "total_pop = sedona.read. \\\n",
    "    format('csv'). \\\n",
    "    load(f'{prefix}' + 'ACSDT5Y2023.B01003_2025-09-19T105050/ACSDT5Y2023.B01003-Data.csv')\n",
    "\n",
    "total_pop.writeTo(f\"wherobots.{database}.total_pop_bronze\").createOrReplace()\n",
    "\n",
    "median_income = sedona.read. \\\n",
    "    format('csv'). \\\n",
    "    load(f'{prefix}' + 'ACSDT5Y2023.B19013_2025-09-19T105253/ACSDT5Y2023.B19013-Data.csv')\n",
    "\n",
    "total_pop.writeTo(f\"wherobots.{database}.median_income_bronze\").createOrReplace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "19610058-ff7d-41fe-807d-7132fec745bb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T02:05:21.706104Z",
     "iopub.status.busy": "2025-10-01T02:05:21.705885Z",
     "iopub.status.idle": "2025-10-01T02:05:22.175298Z",
     "shell.execute_reply": "2025-10-01T02:05:22.174689Z",
     "shell.execute_reply.started": "2025-10-01T02:05:21.706089Z"
    }
   },
   "outputs": [],
   "source": [
    "# Tranist Routes\n",
    "transit_routes = sedona.read. \\\n",
    "    format('shapefile'). \\\n",
    "    load(f'{prefix}' + 'Transit_Routes_for_King_County_Metro___transitroute_line/Transit_Routes_for_King_County_Metro___transitroute_line.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "38fb1716-891b-4882-8cab-6195492dea79",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T02:05:22.748048Z",
     "iopub.status.busy": "2025-10-01T02:05:22.747837Z",
     "iopub.status.idle": "2025-10-01T02:05:29.591537Z",
     "shell.execute_reply": "2025-10-01T02:05:29.590942Z",
     "shell.execute_reply.started": "2025-10-01T02:05:22.748032Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 44:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Checked geometries — found 0 invalid geometries.\n",
      "✅ All geometries are valid.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "result = process_geometries(transit_routes, geom_col=\"geometry\", attempt_fix=True, split_on_fail=True)\n",
    "\n",
    "if \"df\" in result:\n",
    "    df_final = result[\"df\"]  # all valid (either already valid or successfully repaired)\n",
    "else:\n",
    "    valid_df = result[\"valid_df\"]\n",
    "    invalid_df = result[\"invalid_df\"]\n",
    "    # handle invalids (e.g., export for manual review)(fld_hazard_area, 'geometry')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "668665e0-8ca0-47c1-ad0b-6de4aa338d49",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T02:05:29.592416Z",
     "iopub.status.busy": "2025-10-01T02:05:29.592285Z",
     "iopub.status.idle": "2025-10-01T02:05:37.070529Z",
     "shell.execute_reply": "2025-10-01T02:05:37.069752Z",
     "shell.execute_reply.started": "2025-10-01T02:05:29.592403Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_final.writeTo(f\"wherobots.{database}.transit_routes_bronze\").createOrReplace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1b97e87d-4079-4f92-8e35-af08b6d0df95",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T02:06:29.667339Z",
     "iopub.status.busy": "2025-10-01T02:06:29.667128Z",
     "iopub.status.idle": "2025-10-01T02:06:30.214351Z",
     "shell.execute_reply": "2025-10-01T02:06:30.213676Z",
     "shell.execute_reply.started": "2025-10-01T02:06:29.667324Z"
    }
   },
   "outputs": [],
   "source": [
    "# Transit Stops\n",
    "transit_stops = sedona.read. \\\n",
    "    format('shapefile'). \\\n",
    "    load(f'{prefix}' + 'Transit_Stops_for_King_County_Metro___transitstop_point/Transit_Stops_for_King_County_Metro___transitstop_point.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d1b1bb6e-8d28-4247-b3c5-4d7f26630e85",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T02:06:30.318869Z",
     "iopub.status.busy": "2025-10-01T02:06:30.318651Z",
     "iopub.status.idle": "2025-10-01T02:06:38.452927Z",
     "shell.execute_reply": "2025-10-01T02:06:38.452445Z",
     "shell.execute_reply.started": "2025-10-01T02:06:30.318854Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 50:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Checked geometries — found 0 invalid geometries.\n",
      "✅ All geometries are valid.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "result = process_geometries(transit_stops, geom_col=\"geometry\", attempt_fix=True, split_on_fail=True)\n",
    "\n",
    "if \"df\" in result:\n",
    "    df_final = result[\"df\"]  # all valid (either already valid or successfully repaired)\n",
    "else:\n",
    "    valid_df = result[\"valid_df\"]\n",
    "    invalid_df = result[\"invalid_df\"]\n",
    "    # handle invalids (e.g., export for manual review)(fld_hazard_area, 'geometry')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "52c05ba8-9750-41b0-a695-a37fa1b83be5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T02:14:34.379212Z",
     "iopub.status.busy": "2025-10-01T02:14:34.379005Z",
     "iopub.status.idle": "2025-10-01T02:14:41.484285Z",
     "shell.execute_reply": "2025-10-01T02:14:41.483743Z",
     "shell.execute_reply.started": "2025-10-01T02:14:34.379197Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_final.writeTo(f\"wherobots.{database}.transit_stops_bronze\").createOrReplace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3ab603b1-706d-403f-9ba7-bc49820bb224",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T02:15:03.968609Z",
     "iopub.status.busy": "2025-10-01T02:15:03.968392Z",
     "iopub.status.idle": "2025-10-01T02:15:04.526132Z",
     "shell.execute_reply": "2025-10-01T02:15:04.525166Z",
     "shell.execute_reply.started": "2025-10-01T02:15:03.968594Z"
    }
   },
   "outputs": [],
   "source": [
    "# Water Bodies\n",
    "water_bodies = sedona.read. \\\n",
    "    format('shapefile'). \\\n",
    "    load(f'{prefix}' + 'Waterbodies_with_History_and_Jurisdictional_detail___wtrbdy_det_area/Waterbodies_with_History_and_Jurisdictional_detail___wtrbdy_det_area.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "86c0cfe3-14a5-458e-8fd8-69794cc07565",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T02:15:11.617341Z",
     "iopub.status.busy": "2025-10-01T02:15:11.617137Z",
     "iopub.status.idle": "2025-10-01T02:15:28.653662Z",
     "shell.execute_reply": "2025-10-01T02:15:28.653017Z",
     "shell.execute_reply.started": "2025-10-01T02:15:11.617326Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Checked geometries — found 5 invalid geometries.\n",
      "🔧 Attempting to fix 5 invalid geometries...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 60:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔎 After fixing, 0 invalid geometries remain.\n",
      "✅ All geometries are valid after fixing.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "result = process_geometries(water_bodies, geom_col=\"geometry\", attempt_fix=True, split_on_fail=True)\n",
    "\n",
    "if \"df\" in result:\n",
    "    df_final = result[\"df\"]  # all valid (either already valid or successfully repaired)\n",
    "else:\n",
    "    valid_df = result[\"valid_df\"]\n",
    "    invalid_df = result[\"invalid_df\"]\n",
    "    # handle invalids (e.g., export for manual review)(fld_hazard_area, 'geometry')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c96768e9-9c12-4946-ba3e-7d1bb29c271e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T02:15:29.753277Z",
     "iopub.status.busy": "2025-10-01T02:15:29.753044Z",
     "iopub.status.idle": "2025-10-01T02:15:38.216698Z",
     "shell.execute_reply": "2025-10-01T02:15:38.216262Z",
     "shell.execute_reply.started": "2025-10-01T02:15:29.753261Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_final.writeTo(f\"wherobots.{database}.water_bodies_bronze\").createOrReplace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "09f651b2-135e-483a-b376-1722e1b69d08",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T02:17:29.993248Z",
     "iopub.status.busy": "2025-10-01T02:17:29.993038Z",
     "iopub.status.idle": "2025-10-01T02:17:30.401920Z",
     "shell.execute_reply": "2025-10-01T02:17:30.401183Z",
     "shell.execute_reply.started": "2025-10-01T02:17:29.993233Z"
    }
   },
   "outputs": [],
   "source": [
    "# Wildfire Polygons\n",
    "wildfires = sedona.read. \\\n",
    "    format('shapefile'). \\\n",
    "    load(f'{prefix}' + 'Wildfires_1878_2019_Polygon_Data/Shapefile/US_Wildfires_1878_2019.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3b275ad2-5cc6-4825-a123-7179a0b56928",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T02:17:33.479757Z",
     "iopub.status.busy": "2025-10-01T02:17:33.479527Z",
     "iopub.status.idle": "2025-10-01T02:18:15.500275Z",
     "shell.execute_reply": "2025-10-01T02:18:15.499882Z",
     "shell.execute_reply.started": "2025-10-01T02:17:33.479741Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Checked geometries — found 1203 invalid geometries.\n",
      "🔧 Attempting to fix 1203 invalid geometries...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 70:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔎 After fixing, 0 invalid geometries remain.\n",
      "✅ All geometries are valid after fixing.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "result = process_geometries(wildfires, geom_col=\"geometry\", attempt_fix=True, split_on_fail=True)\n",
    "\n",
    "if \"df\" in result:\n",
    "    df_final = result[\"df\"]  # all valid (either already valid or successfully repaired)\n",
    "else:\n",
    "    valid_df = result[\"valid_df\"]\n",
    "    invalid_df = result[\"invalid_df\"]\n",
    "    # handle invalids (e.g., export for manual review)(fld_hazard_area, 'geometry')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f036a2b1-a97d-4731-ad41-fd73d87296f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T02:18:15.501046Z",
     "iopub.status.busy": "2025-10-01T02:18:15.500903Z",
     "iopub.status.idle": "2025-10-01T02:18:38.684490Z",
     "shell.execute_reply": "2025-10-01T02:18:38.683764Z",
     "shell.execute_reply.started": "2025-10-01T02:18:15.501032Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_final.writeTo(f\"wherobots.{database}.wildfires_bronze\").createOrReplace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dddd5cd5-16a2-408b-942e-0e8a86126ef4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-30T19:23:25.942472Z",
     "iopub.status.busy": "2025-09-30T19:23:25.942278Z",
     "iopub.status.idle": "2025-09-30T19:23:25.944744Z",
     "shell.execute_reply": "2025-09-30T19:23:25.944439Z",
     "shell.execute_reply.started": "2025-09-30T19:23:25.942459Z"
    }
   },
   "outputs": [],
   "source": [
    "# Wildfire Rasters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "278f504d-8689-4c71-bbb1-bf9d2697dab7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-30T19:23:42.708892Z",
     "iopub.status.busy": "2025-09-30T19:23:42.708684Z",
     "iopub.status.idle": "2025-09-30T19:23:42.710991Z",
     "shell.execute_reply": "2025-09-30T19:23:42.710638Z",
     "shell.execute_reply.started": "2025-09-30T19:23:42.708878Z"
    }
   },
   "outputs": [],
   "source": [
    "# Elevation\n",
    "\n",
    "# https://s3.opengeohub.org/global/edtm/gedtm_rf_m_30m_s_20060101_20151231_go_epsg.4326.3855_v20250611.tif\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8270605c-65ae-4222-8975-554d17b03b53",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T02:26:56.493984Z",
     "iopub.status.busy": "2025-10-01T02:26:56.493765Z",
     "iopub.status.idle": "2025-10-01T02:26:57.043200Z",
     "shell.execute_reply": "2025-10-01T02:26:57.042663Z",
     "shell.execute_reply.started": "2025-10-01T02:26:56.493969Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Geocoded Schools\n",
    "schools = sedona.read. \\\n",
    "    format('geojson'). \\\n",
    "    load(f'{prefix}' + 'Washington_State_Public_Schools_GeoCoded.geojson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "34c889f2-9210-4c03-b6a1-5357929ea913",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T02:29:02.255826Z",
     "iopub.status.busy": "2025-10-01T02:29:02.255609Z",
     "iopub.status.idle": "2025-10-01T02:29:02.410215Z",
     "shell.execute_reply": "2025-10-01T02:29:02.409726Z",
     "shell.execute_reply.started": "2025-10-01T02:29:02.255809Z"
    }
   },
   "outputs": [],
   "source": [
    "schools = schools \\\n",
    "    .withColumn(\"geometry\", expr(\"geometry\")) \\\n",
    "    .withColumn(\"AYPCode\", expr(\"properties['AYPCode']\")) \\\n",
    "    .withColumn(\"CongressionalDistrict\", expr(\"properties['CongressionalDistrict']\")) \\\n",
    "    .withColumn(\"County\", expr(\"properties['County']\")) \\\n",
    "    .withColumn(\"ESDCode\", expr(\"properties['ESDCode']\")) \\\n",
    "    .withColumn(\"ESDName\", expr(\"properties['ESDName']\")) \\\n",
    "    .withColumn(\"Email\", expr(\"properties['Email']\")) \\\n",
    "    .withColumn(\"GeoCoded_X\", expr(\"properties['GeoCoded_X']\")) \\\n",
    "    .withColumn(\"GeoCoded_Y\", expr(\"properties['GeoCoded_Y']\")) \\\n",
    "    .withColumn(\"GradeCategory\", expr(\"properties['GradeCategory']\")) \\\n",
    "    .withColumn(\"HighestGrade\", expr(\"properties['HighestGrade']\")) \\\n",
    "    .withColumn(\"LEACode\", expr(\"properties['LEACode']\")) \\\n",
    "    .withColumn(\"LEAName\", expr(\"properties['LEAName']\")) \\\n",
    "    .withColumn(\"LegislativeDistrict\", expr(\"properties['LegislativeDistrict']\")) \\\n",
    "    .withColumn(\"LowestGrade\", expr(\"properties['LowestGrade']\")) \\\n",
    "    .withColumn(\"MailingAddress\", expr(\"properties['MailingAddress']\")) \\\n",
    "    .withColumn(\"NCES_X\", expr(\"properties['NCES_X']\")) \\\n",
    "    .withColumn(\"NCES_Y\", expr(\"properties['NCES_Y']\")) \\\n",
    "    .withColumn(\"Phone\", expr(\"properties['Phone']\")) \\\n",
    "    .withColumn(\"Principal\", expr(\"properties['Principal']\")) \\\n",
    "    .withColumn(\"School\", expr(\"properties['School']\")) \\\n",
    "    .withColumn(\"SchoolCategory\", expr(\"properties['SchoolCategory']\")) \\\n",
    "    .withColumn(\"SchoolCode\", expr(\"properties['SchoolCode']\")) \\\n",
    "    .withColumn(\"SingleAddress\", expr(\"properties['SingleAddress']\")) \\\n",
    "    .drop(\"properties\").drop(\"type\") \\\n",
    "    .drop(\"_corrupt_record\").drop(\"type\") \\\n",
    "    .drop(\"type\").drop(\"type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f8ce1324-82c6-4300-8389-70e3b4935044",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T02:30:15.910373Z",
     "iopub.status.busy": "2025-10-01T02:30:15.910144Z",
     "iopub.status.idle": "2025-10-01T02:30:15.914331Z",
     "shell.execute_reply": "2025-10-01T02:30:15.913846Z",
     "shell.execute_reply.started": "2025-10-01T02:30:15.910357Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- geometry: geometry (nullable = true)\n",
      " |-- AYPCode: string (nullable = true)\n",
      " |-- CongressionalDistrict: string (nullable = true)\n",
      " |-- County: string (nullable = true)\n",
      " |-- ESDCode: long (nullable = true)\n",
      " |-- ESDName: string (nullable = true)\n",
      " |-- Email: string (nullable = true)\n",
      " |-- GeoCoded_X: double (nullable = true)\n",
      " |-- GeoCoded_Y: double (nullable = true)\n",
      " |-- GradeCategory: string (nullable = true)\n",
      " |-- HighestGrade: string (nullable = true)\n",
      " |-- LEACode: long (nullable = true)\n",
      " |-- LEAName: string (nullable = true)\n",
      " |-- LegislativeDistrict: string (nullable = true)\n",
      " |-- LowestGrade: string (nullable = true)\n",
      " |-- MailingAddress: string (nullable = true)\n",
      " |-- NCES_X: double (nullable = true)\n",
      " |-- NCES_Y: double (nullable = true)\n",
      " |-- Phone: string (nullable = true)\n",
      " |-- Principal: string (nullable = true)\n",
      " |-- School: string (nullable = true)\n",
      " |-- SchoolCategory: string (nullable = true)\n",
      " |-- SchoolCode: long (nullable = true)\n",
      " |-- SingleAddress: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schools.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "004fd9c5-5174-4642-9472-f06df63ed46e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T02:29:05.808796Z",
     "iopub.status.busy": "2025-10-01T02:29:05.808592Z",
     "iopub.status.idle": "2025-10-01T02:29:07.004511Z",
     "shell.execute_reply": "2025-10-01T02:29:07.003843Z",
     "shell.execute_reply.started": "2025-10-01T02:29:05.808781Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/01 02:29:06 ERROR TaskSetManager: Task 0 in stage 84.0 failed 4 times; aborting job\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o494.count.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 84.0 failed 4 times, most recent failure: Lost task 0.3 in stage 84.0 (TID 1698) (10.1.31.162 executor 8): java.lang.NullPointerException: Cannot invoke \"org.apache.spark.unsafe.types.UTF8String.toString()\" because the return value of \"org.apache.spark.sql.catalyst.InternalRow.getUTF8String(int)\" is null\n\tat org.apache.spark.sql.catalyst.InternalRow.getString(InternalRow.scala:35)\n\tat org.apache.spark.sql.sedona_sql.io.geojson.GeoJSONUtils$.$anonfun$convertGeoJsonToGeometry$1(GeoJSONUtils.scala:111)\n\tat org.apache.spark.sql.sedona_sql.io.geojson.GeoJSONUtils$.$anonfun$convertGeoJsonToGeometry$1$adapted(GeoJSONUtils.scala:109)\n\tat scala.collection.ArrayOps$.foreach$extension(ArrayOps.scala:1328)\n\tat org.apache.spark.sql.sedona_sql.io.geojson.GeoJSONUtils$.convertGeoJsonToGeometry(GeoJSONUtils.scala:109)\n\tat org.apache.spark.sql.sedona_sql.io.geojson.GeoJSONFileFormat.$anonfun$buildReader$3(GeoJSONFileFormat.scala:181)\n\tat scala.collection.Iterator$$anon$9.next(Iterator.scala:577)\n\tat scala.collection.Iterator$$anon$9.next(Iterator.scala:577)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.next(FileScanRDD.scala:199)\n\tat scala.collection.Iterator$$anon$9.next(Iterator.scala:577)\n\tat scala.collection.Iterator$$anon$9.next(Iterator.scala:577)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:90)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:80)\n\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:290)\n\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:287)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:224)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1597)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1524)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1588)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:379)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.immutable.List.foreach(List.scala:333)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:437)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\nCaused by: java.lang.NullPointerException\n\tat org.apache.spark.sql.catalyst.InternalRow.getString(InternalRow.scala:35)\n\tat org.apache.spark.sql.sedona_sql.io.geojson.GeoJSONUtils$.$anonfun$convertGeoJsonToGeometry$1(GeoJSONUtils.scala:111)\n\tat org.apache.spark.sql.sedona_sql.io.geojson.GeoJSONUtils$.$anonfun$convertGeoJsonToGeometry$1$adapted(GeoJSONUtils.scala:109)\n\tat scala.collection.ArrayOps$.foreach$extension(ArrayOps.scala:1328)\n\tat org.apache.spark.sql.sedona_sql.io.geojson.GeoJSONUtils$.convertGeoJsonToGeometry(GeoJSONUtils.scala:109)\n\tat org.apache.spark.sql.sedona_sql.io.geojson.GeoJSONFileFormat.$anonfun$buildReader$3(GeoJSONFileFormat.scala:181)\n\tat scala.collection.Iterator$$anon$9.next(Iterator.scala:577)\n\tat scala.collection.Iterator$$anon$9.next(Iterator.scala:577)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.next(FileScanRDD.scala:199)\n\tat scala.collection.Iterator$$anon$9.next(Iterator.scala:577)\n\tat scala.collection.Iterator$$anon$9.next(Iterator.scala:577)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:90)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:80)\n\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:290)\n\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:287)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:224)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1597)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1524)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1588)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:379)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPy4JJavaError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[55]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m result = \u001b[43mprocess_geometries\u001b[49m\u001b[43m(\u001b[49m\u001b[43mschools\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgeom_col\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgeometry\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattempt_fix\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit_on_fail\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mdf\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m result:\n\u001b[32m      4\u001b[39m     df_final = result[\u001b[33m\"\u001b[39m\u001b[33mdf\u001b[39m\u001b[33m\"\u001b[39m]  \u001b[38;5;66;03m# all valid (either already valid or successfully repaired)\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 34\u001b[39m, in \u001b[36mprocess_geometries\u001b[39m\u001b[34m(df, geom_col, attempt_fix, split_on_fail)\u001b[39m\n\u001b[32m     27\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     28\u001b[39m \u001b[33;03mRuns validity check -> optional repair -> optional split.\u001b[39;00m\n\u001b[32m     29\u001b[39m \u001b[33;03mReturns either:\u001b[39;00m\n\u001b[32m     30\u001b[39m \u001b[33;03m  - {\"df\": corrected_df}  when all geometries valid after repair (or none invalid)\u001b[39;00m\n\u001b[32m     31\u001b[39m \u001b[33;03m  - {\"valid_df\": ..., \"invalid_df\": ...} when some invalid remain and split_on_fail=True\u001b[39;00m\n\u001b[32m     32\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m# 1) Initial check\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m invalid_count = \u001b[43mcheck_invalid_geometries\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgeom_col\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeom_col\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m invalid_count == \u001b[32m0\u001b[39m:\n\u001b[32m     37\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m✅ All geometries are valid.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 5\u001b[39m, in \u001b[36mcheck_invalid_geometries\u001b[39m\u001b[34m(df, geom_col, reason_col)\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# cache to avoid recomputation if you inspect reasons later\u001b[39;00m\n\u001b[32m      4\u001b[39m df_with_reason.cache()\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m invalid_count = \u001b[43mdf_with_reason\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfilter\u001b[49m\u001b[43m(\u001b[49m\u001b[43m~\u001b[49m\u001b[43mST_IsValid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgeom_col\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m✅ Checked geometries — found \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minvalid_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m invalid geometries.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m invalid_count\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/spark/python/pyspark/sql/dataframe.py:1240\u001b[39m, in \u001b[36mDataFrame.count\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1217\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcount\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28mint\u001b[39m:\n\u001b[32m   1218\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Returns the number of rows in this :class:`DataFrame`.\u001b[39;00m\n\u001b[32m   1219\u001b[39m \n\u001b[32m   1220\u001b[39m \u001b[33;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1238\u001b[39m \u001b[33;03m    3\u001b[39;00m\n\u001b[32m   1239\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1240\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/wherobots/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1316\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1317\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1318\u001b[39m     args_command +\\\n\u001b[32m   1319\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1321\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1323\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/spark/python/pyspark/errors/exceptions/captured.py:179\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdeco\u001b[39m(*a: Any, **kw: Any) -> Any:\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    180\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    181\u001b[39m         converted = convert_exception(e.java_exception)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/wherobots/lib/python3.11/site-packages/py4j/protocol.py:326\u001b[39m, in \u001b[36mget_return_value\u001b[39m\u001b[34m(answer, gateway_client, target_id, name)\u001b[39m\n\u001b[32m    324\u001b[39m value = OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[32m2\u001b[39m:], gateway_client)\n\u001b[32m    325\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[32m1\u001b[39m] == REFERENCE_TYPE:\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[32m    327\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    328\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name), value)\n\u001b[32m    329\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    330\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[32m    331\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    332\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name, value))\n",
      "\u001b[31mPy4JJavaError\u001b[39m: An error occurred while calling o494.count.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 84.0 failed 4 times, most recent failure: Lost task 0.3 in stage 84.0 (TID 1698) (10.1.31.162 executor 8): java.lang.NullPointerException: Cannot invoke \"org.apache.spark.unsafe.types.UTF8String.toString()\" because the return value of \"org.apache.spark.sql.catalyst.InternalRow.getUTF8String(int)\" is null\n\tat org.apache.spark.sql.catalyst.InternalRow.getString(InternalRow.scala:35)\n\tat org.apache.spark.sql.sedona_sql.io.geojson.GeoJSONUtils$.$anonfun$convertGeoJsonToGeometry$1(GeoJSONUtils.scala:111)\n\tat org.apache.spark.sql.sedona_sql.io.geojson.GeoJSONUtils$.$anonfun$convertGeoJsonToGeometry$1$adapted(GeoJSONUtils.scala:109)\n\tat scala.collection.ArrayOps$.foreach$extension(ArrayOps.scala:1328)\n\tat org.apache.spark.sql.sedona_sql.io.geojson.GeoJSONUtils$.convertGeoJsonToGeometry(GeoJSONUtils.scala:109)\n\tat org.apache.spark.sql.sedona_sql.io.geojson.GeoJSONFileFormat.$anonfun$buildReader$3(GeoJSONFileFormat.scala:181)\n\tat scala.collection.Iterator$$anon$9.next(Iterator.scala:577)\n\tat scala.collection.Iterator$$anon$9.next(Iterator.scala:577)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.next(FileScanRDD.scala:199)\n\tat scala.collection.Iterator$$anon$9.next(Iterator.scala:577)\n\tat scala.collection.Iterator$$anon$9.next(Iterator.scala:577)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:90)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:80)\n\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:290)\n\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:287)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:224)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1597)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1524)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1588)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:379)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.immutable.List.foreach(List.scala:333)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:437)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\nCaused by: java.lang.NullPointerException\n\tat org.apache.spark.sql.catalyst.InternalRow.getString(InternalRow.scala:35)\n\tat org.apache.spark.sql.sedona_sql.io.geojson.GeoJSONUtils$.$anonfun$convertGeoJsonToGeometry$1(GeoJSONUtils.scala:111)\n\tat org.apache.spark.sql.sedona_sql.io.geojson.GeoJSONUtils$.$anonfun$convertGeoJsonToGeometry$1$adapted(GeoJSONUtils.scala:109)\n\tat scala.collection.ArrayOps$.foreach$extension(ArrayOps.scala:1328)\n\tat org.apache.spark.sql.sedona_sql.io.geojson.GeoJSONUtils$.convertGeoJsonToGeometry(GeoJSONUtils.scala:109)\n\tat org.apache.spark.sql.sedona_sql.io.geojson.GeoJSONFileFormat.$anonfun$buildReader$3(GeoJSONFileFormat.scala:181)\n\tat scala.collection.Iterator$$anon$9.next(Iterator.scala:577)\n\tat scala.collection.Iterator$$anon$9.next(Iterator.scala:577)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.next(FileScanRDD.scala:199)\n\tat scala.collection.Iterator$$anon$9.next(Iterator.scala:577)\n\tat scala.collection.Iterator$$anon$9.next(Iterator.scala:577)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:90)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:80)\n\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:290)\n\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:287)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:224)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1597)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1524)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1588)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:379)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n"
     ]
    }
   ],
   "source": [
    "result = process_geometries(schools, geom_col=\"geometry\", attempt_fix=True, split_on_fail=True)\n",
    "\n",
    "if \"df\" in result:\n",
    "    df_final = result[\"df\"]  # all valid (either already valid or successfully repaired)\n",
    "else:\n",
    "    valid_df = result[\"valid_df\"]\n",
    "    invalid_df = result[\"invalid_df\"]\n",
    "    # handle invalids (e.g., export for manual review)(fld_hazard_area, 'geometry')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ac3d21-b73c-475c-b3f0-e8254de09b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.writeTo(f\"wherobots.{database}.schools_bronze\").createOrReplace()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
