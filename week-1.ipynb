{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cacaa5d7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Overview of geospatial data, coordinate systems and geometry types.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e3100f77-fe3f-4c9a-8a0a-a4d62c723e50",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T02:28:20.837341Z",
     "iopub.status.busy": "2025-10-01T02:28:20.837117Z",
     "iopub.status.idle": "2025-10-01T02:28:20.840852Z",
     "shell.execute_reply": "2025-10-01T02:28:20.840233Z",
     "shell.execute_reply.started": "2025-10-01T02:28:20.837325Z"
    }
   },
   "outputs": [],
   "source": [
    "from sedona.spark import *\n",
    "from pyspark.sql.functions import col, when, expr\n",
    "from sedona.sql.st_functions import ST_IsValid, ST_IsValidReason, ST_MakeValid\n",
    "from pyspark.sql import DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8625818a-f65d-44c6-a5f2-a7d149c5075b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T01:50:33.045978Z",
     "iopub.status.busy": "2025-10-01T01:50:33.045621Z",
     "iopub.status.idle": "2025-10-01T01:53:07.039846Z",
     "shell.execute_reply": "2025-10-01T01:53:07.039072Z",
     "shell.execute_reply.started": "2025-10-01T01:50:33.045959Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "config = SedonaContext.builder().getOrCreate()\n",
    "sedona = SedonaContext.create(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2e45a109-8ccf-491f-9902-b8d2e185b025",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-30T18:18:29.809907Z",
     "iopub.status.busy": "2025-09-30T18:18:29.809687Z",
     "iopub.status.idle": "2025-09-30T18:18:29.812435Z",
     "shell.execute_reply": "2025-09-30T18:18:29.812062Z",
     "shell.execute_reply.started": "2025-09-30T18:18:29.809878Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78e0763",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Read and create basic data frames  - Pranav\n",
    "\n",
    "sedona.read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9246ee42",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Vector drivers  - Pranav\n",
    "\n",
    "# GeoJSON\n",
    "# CSV\n",
    "# Shapefile\n",
    "\n",
    "# Show others in comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9e1a44",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Raster - Pranav\n",
    "\n",
    "# Raster\n",
    "\n",
    "# Show others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c85572",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Intro to cloud native formats - Pranav\n",
    "\n",
    "# Demo the speed on this \n",
    "# COG from LANDSAT - STAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45d41f0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Intro to Iceberg - Pranav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fba1939",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Transforming data with non-native readers - in slides"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010f993a",
   "metadata": {},
   "source": [
    "# Wherobots Fundamentals - Constructing\n",
    "\n",
    "WherobotsDB provides a powerful set of functions to construct geometries. You can either create them from scratch using raw coordinate values (literals) or by parsing standard geospatial data formats like WKT and WKB.\n",
    "\n",
    "---\n",
    "\n",
    "### Creating from Coordinates\n",
    "\n",
    "These functions build geometries directly from numerical inputs.\n",
    "\n",
    "* `ST_MakePoint(x, y, [z], [m])`: Creates a **Point** geometry from its x and y coordinates. You can also optionally provide a z-coordinate (for elevation) and an m-coordinate (a measure value).\n",
    "\n",
    "* `ST_MakeEnvelope(xmin, ymin, xmax, ymax)`: Creates a rectangular **Polygon** that represents a bounding box, or \"envelope,\" from the coordinates of two opposing corners.\n",
    "\n",
    "* `ST_LineStringFromText(text, delimiter)`: This function builds a LineString from a flat string of comma-separated coordinates, like `'x1, y1, x2, y2, ...'`. This provides a fast way to create line geometries directly from raw text data without needing the formal structure of WKT.\n",
    "\n",
    "* `ST_PolygonFromText(text, delimiter)`: Similarly, the ST_PolygonFromText function creates a Polygon from a flat string of comma-separated coordinates. For a valid polygon, the sequence must form a closed ring by ensuring the last coordinate pair is identical to the first (e.g., `'x1, y1, x2, y2, x3, y3, x1, y1'`).\n",
    "\n",
    "---\n",
    "\n",
    "### Creating from Standard Formats\n",
    "\n",
    "These functions parse common text-based or binary geospatial formats.\n",
    "\n",
    "* `ST_GeomFromWKT(text)`: The primary function for constructing any geometry type from its **W**ell-**K**nown **T**ext (WKT) representation. This is one of the most common ways to ingest geometries.\n",
    "\n",
    "* `ST_GeomFromWKB(binary)`: Creates a geometry from its **W**ell-**K**nown **B**inary (WKB) representation, which is a compact, machine-readable alternative to WKT.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Creating from Other Geometries\n",
    "\n",
    "This function combines existing geometries into a single feature.\n",
    "\n",
    "* `ST_Collect(geometry_array)`: Takes an array of geometries and aggregates them into a single multi-part geometry (e.g., `MultiPoint`, `MultiPolygon`) or a `GeometryCollection`. This is useful for grouping related features together. This is an essential function because direct spatial operations on arrays are often limited, so ST_Collect consolidates the individual geometries into one object that can then be analyzed.\n",
    "\n",
    "---\n",
    "\n",
    "While these are common examples, they are not the only constructor functions available in WherobotsDB. We will now look at examples for `ST_MakePoint`, `ST_LineStringFromText`, `ST_Collect`, and `ST_MakeEnvelope`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97066a60",
   "metadata": {},
   "source": [
    "### ST_MakePoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0658995c",
   "metadata": {},
   "outputs": [],
   "source": [
    "points_df = sedona.sql(\"\"\"\n",
    "\n",
    "SELECT ST_MakePoint(-122.349277, 47.620504) as space_needle, ST_MakePoint(-122.350446, 47.620556) as glass_museum, ST_MakePoint(-122.348258, 47.621494) as pop_culture_museum\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "points_df.show(1, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362730b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_config_url = \"https://raw.githubusercontent.com/wherobots/geospatial-data-engineering-associate/refs/heads/main/assets/week-1/conf/map_config.json\"\n",
    "\n",
    "with open(map_config_url, 'r') as file:\n",
    "    map_config = json.load(file)\n",
    "\n",
    "map = SedonaKepler.create_map(points_df, \"Tourist spots\", map_config)\n",
    "map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d1c3de",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### ST_LineStringFromText()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5cf8b5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "line_df = sedona.sql(\"\"\"\n",
    "\n",
    "SELECT ST_LineStringFromText('-122.349277, 47.620504, -122.350446, 47.620556, -122.348258, 47.621494', ',') as order_to_visit\n",
    "\n",
    "\"\"\")\n",
    "line_df.show(1, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a6e679",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### ST_Collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d37bee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PS. this allows us to access the dataframes in a SQL environment\n",
    "points_df.createOrReplaceTempView(\"points\")\n",
    "line_df.createOrReplaceTempView(\"line\")\n",
    "\n",
    "collection_df = sedona.sql(\"\"\"\n",
    "\n",
    "SELECT \n",
    "    ST_Collect(Array(space_needle, glass_museum, pop_culture_museum, order_to_visit))\n",
    "FROM points, line\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "collection_df.show(1, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ca298d",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_collection = SedonaKepler.create_map(collection_df, \"Things to do\", map_config)\n",
    "map_collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a248f2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### ST_MakeEnvelope()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a27d997",
   "metadata": {},
   "outputs": [],
   "source": [
    "envelope_df = sedona.sql(\"\"\"\n",
    "\n",
    "SELECT ST_MakeEnvelope(-122.352848,47.619674,-122.346539,47.622451) AS tourist_location_bbox\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "envelope_df.show(1, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c84716a",
   "metadata": {},
   "source": [
    "# Wherobots Fundamentals - Spatial Predicates\n",
    "\n",
    "Spatial predicates are functions that test the relationship between two geometries, returning `TRUE` or `FALSE`. They form the core of most spatial analysis, allowing you to filter data or create joins based on how geometries interact with each other. Understanding the exact logic of each predicate is key to performing accurate analysis.\n",
    "\n",
    "In this section, we will explore some of the most essential predicate functions in detail.\n",
    "\n",
    "-----\n",
    "\n",
    "## ST_Intersects(A, B)\n",
    "\n",
    "This is the most general-purpose spatial relationship, returning `TRUE` if two geometries **share any space at all**. This includes touching at a single point on their boundaries or overlapping in any way. It's the opposite of `ST_Disjoint`.\n",
    "\n",
    "  * **Use Case:** Finding any parcels that intersect with a specific road.\n",
    "\n",
    "-----\n",
    "\n",
    "## ST_Contains(A, B) and ST_Within(A, B)\n",
    "\n",
    "These two functions are opposites and describe a \"spatially-inside\" relationship.\n",
    "\n",
    "  * `ST_Contains(A, B)` returns `TRUE` if geometry **A** completely encloses geometry **B**. No part of B can be outside of A. Think of a cookie inside a cookie jar; the jar contains the cookie.\n",
    "  * `ST_Within(A, B)` returns `TRUE` if geometry **A** is completely inside geometry **B**. It's the reverse of `ST_Contains`. The cookie is within the jar.\n",
    "\n",
    "A key detail is that the boundaries of the geometries cannot simply touch; at least one point of the inner geometry's interior must fall inside the outer geometry's interior. For example, a line that lies perfectly on the boundary of a polygon is not *contained* by it.\n",
    "\n",
    "  * **Use Case:** Finding all the schools (`ST_Within`) a specific city district (`ST_Contains`).\n",
    "\n",
    "-----\n",
    "\n",
    "## ST_Overlaps(A, B)\n",
    "\n",
    "This predicate is more specific than `ST_Intersects`. It returns `TRUE` only if two geometries **partially intersect** and are of the **same dimension**. For example, two overlapping polygons will return `TRUE`, but a line crossing a polygon will not. Critically, neither geometry can be completely contained within the other.\n",
    "\n",
    "  * **Use Case:** Finding sales regions that have a partial overlap, which might indicate a territory dispute.\n",
    "\n",
    "-----\n",
    "\n",
    "### ST_DWithin(A, B, distance, [useSpheroid])\n",
    "\n",
    "Instead of just testing a direct spatial relationship, `ST_DWithin` checks for **proximity**. It returns `TRUE` if the two geometries are **within a specified distance** of each other. This is extremely powerful for \"buffer\" style queries and is highly optimized to use spatial indexes, making it much faster than calculating the exact distance for every pair of geometries.\n",
    "\n",
    "#### Distance Calculation: Spheroid vs. Euclidean\n",
    "\n",
    "The optional `useSpheroid` flag is crucial as it controls how the distance is calculated:\n",
    "\n",
    "* **`useSpheroid = true` (Spheroidal Distance):** This method should be used for geographic data (latitude/longitude). It calculates the more accurate \"great-circle\" distance on a curved surface. When this is enabled, the distance unit is always in **meters**, and the calculation is performed between the centroids of the two geometries.\n",
    "\n",
    "* **`useSpheroid = false` (Euclidean Distance):** This is the default behavior. It performs a simpler, \"flat-earth\" distance calculation. The unit of the `distance` parameter in this case is the same as the unit of the data's Coordinate Reference System (CRS). For accurate results, you should first transform your data into a projected CRS appropriate for distance measurements (e.g., a UTM or State Plane system).\n",
    "\n",
    "* **Use Case:** Finding all ATMs within 500 **meters** of a specific address using geographic coordinates (`useSpheroid = true`), or finding all competing stores within 2,500 **feet** of a location using a projected State Plane CRS where the units are in feet (`useSpheroid = false`).\n",
    "\n",
    "-----\n",
    "\n",
    "These are just a few of the many powerful predicate functions available in WherobotsDB. You can find the complete list in the [official documentation](https://docs.wherobots.com/latest/references/wherobotsdb/vector-data/Predicate/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf7f3c7",
   "metadata": {},
   "source": [
    "# Wherobots Fundamentals - Spatial Joins (Range Joins)\n",
    "\n",
    "Now that you understand spatial predicates, you can use them to perform one of the most powerful operations in geospatial analysis: the **spatial join**. While a standard join uses a key like an ID to match rows (`tableA.id = tableB.id`), a spatial join combines data from two tables based on the spatial relationship between their geometries. This is a type of \"range join\" where the condition isn't simple equality but a spatial test, such as `ST_Intersects(A.geom, B.geom)`.\n",
    "\n",
    "The primary goal of a spatial join is enrichment: adding attributes from one spatial dataset to another based on their shared location.\n",
    "\n",
    "---\n",
    "\n",
    "## Why Spatial Joins Matter\n",
    "\n",
    "Spatial joins allow you to get insights from your data **as if location itself were a column you could join on**. They let you combine completely different datasets using their shared space as the common link, which unlocks powerful analytical capabilities.\n",
    "\n",
    "### Contextual Enrichment\n",
    "\n",
    "Imagine you have a table of customer addresses (points) and a separate table of county demographics (polygons). These tables have no common ID column. A spatial join lets you \"enrich\" your customer data by transferring the demographic information from the county polygon that each customer point falls within. You could then analyze customer behavior by county income level, population density, or any other demographic metric.\n",
    "\n",
    "\n",
    "\n",
    "### Answering Complex Questions\n",
    "\n",
    "Ultimately, spatial joins are how you answer real-world questions that involve location. For example:\n",
    "* Which of our stores are located in flood-prone areas?\n",
    "* What is the average property value for parcels within 500 meters of a new transit line?\n",
    "* How many competitors are within a 10-minute drive of each of our locations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea64a23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "places_df = (\n",
    "    sedona.sql(\"\"\"\n",
    "\n",
    "    WITH seattle_downtown AS (\n",
    "        SELECT ST_GeomFromWKT('POLYGON ((-122.360916 47.590189, -122.299461 47.590189, -122.299461 47.641104, -122.360916 47.641104, -122.360916 47.590189))') AS geom\n",
    "    ),\n",
    "\n",
    "    -- This is to leverage spatial predicate pushdown\n",
    "    places AS (\n",
    "      SELECT *\n",
    "      FROM\n",
    "        wherobots_open_data.overture_maps_foundation.places_place places, seattle_downtown\n",
    "      WHERE\n",
    "        ST_Intersects(places.geometry, seattle_downtown.geom)\n",
    "    ),\n",
    "\n",
    "    -- This is to leverage spatial predicate pushdown\n",
    "    buildings AS (\n",
    "      SELECT *\n",
    "      FROM\n",
    "        wherobots_open_data.overture_maps_foundation.buildings_building buildings, seattle_downtown\n",
    "      WHERE\n",
    "        ST_Intersects(buildings.geometry, seattle_downtown.geom)\n",
    "    )\n",
    "    \n",
    "    SELECT\n",
    "      places.names.primary as place_name,\n",
    "      places.categories.primary as place_type,\n",
    "      element_at(places.addresses, 1) as place_address,\n",
    "      ROUND(places.confidence * 100, 2) AS `place_confidence (%)`,\n",
    "      places.geometry as place_geometry,\n",
    "      buildings.geometry as building_geometry\n",
    "    FROM\n",
    "      places\n",
    "    JOIN\n",
    "      buildings\n",
    "    ON\n",
    "      ST_Intersects(places.geometry, buildings.geometry);\n",
    "    \n",
    "    \"\"\")\n",
    "    # We are caching the result, as we will reuse it to visualize the data\n",
    "        .cache()\n",
    ")\n",
    "\n",
    "places_df.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ce0db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_config_join_url = \"https://raw.githubusercontent.com/wherobots/geospatial-data-engineering-associate/refs/heads/main/assets/week-1/conf/map_config_join.json\"\n",
    "\n",
    "with open(map_config_url, 'r') as file:\n",
    "    map_config = json.load(file)\n",
    "\n",
    "map_places = SedonaKepler.create_map(points_df, \"Places in buildings\", map_config)\n",
    "map_places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62fbfc11",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# create and manage Havasu (Iceberg) tables for vector and raster data  - Furqaan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c14872c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Creating Havasu Tables from Sedona DataFrames\n",
    "\n",
    "Now that you understand the importance of Apache Iceberg, let's see how simple it is to save a Sedona DataFrame as an Iceberg table. Wherobots uses an enhanced version of Iceberg called **Havasu**, which is purpose-built for high-performance geospatial analytics.\n",
    "\n",
    "---\n",
    "\n",
    "## What is Havasu?\n",
    "\n",
    "Standard Apache Iceberg did not natively support geometry data types until its v3 specification. **Havasu** is Wherobots' enhanced implementation of Iceberg that adds first-class support for both **vector** and **raster** data. This allows you to combine all the benefits of the Iceberg format—like atomic transactions and schema evolution—with native, high-performance geospatial data handling.\n",
    "\n",
    "---\n",
    "\n",
    "## Saving a DataFrame to Havasu\n",
    "\n",
    "Saving a Sedona DataFrame as a Havasu table is a straightforward, one-line command. The process is identical whether your DataFrame contains vector geometries, rasters, or no spatial data at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53d025a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Create a new Havasu (Iceberg) database\n",
    "\n",
    "database = 'gde_bronze'\n",
    "\n",
    "sedona.sql(f'CREATE DATABASE IF NOT EXISTS wherobots.{database}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635b0c32",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "geotiff_path = \"s3://wherobots-examples/data/ghs_population/GHS_POP_E1975_GLOBE_R2023A_4326_3ss_V1_0.tif\"\n",
    "\n",
    "df = sedona.read.format(\"raster\") \\\n",
    "    .option(\"tileWidth\", \"512\") \\\n",
    "    .option(\"tileHeight\", \"512\") \\\n",
    "    .option(\"retile\", \"true\") \\\n",
    "    .load(geotiff_path)\n",
    "\n",
    "df.writeTo(f\"wherobots.{database}.ghs_population_tiles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f13332e",
   "metadata": {},
   "source": [
    "# Data validity checks\n",
    "\n",
    "Two of the most common issues with geospatial data include managing projections or Coordinate Reference Systems (CRS) and ensuring geometries are valid.\n",
    "\n",
    "- A geometry is invalid if it violates spatial rules like self-intersections, unclosed rings, misaligned holes, or overlapping parts—making it topologically incorrect.\n",
    "- Spatial files generally contain a Coordinate Reference System or CRS that is defined by a Spatial Reference ID or SRID. This tells us how the data is projected from the round spheroid of the earth onto a flat surface.\n",
    "\n",
    "To fix these issues and ensure our data is valid and in the correct format we use two approaches:\n",
    "\n",
    "1. Check the geometries for any invalidities, and if there are attempt to fix them using `ST_IsValid`, `ST_IsValidDetail`, and `ST_MakeValid`\n",
    "2. Remove or log out any geometries that cannot be fixed\n",
    "3. Standardize our geometries in a single CRS, in this case [EPSG:4326](https://epsg.io/4326) which renders in a coordinate reference system"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54fb11ae",
   "metadata": {},
   "source": [
    "## Validating geometries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf2ec1a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Data validity checks - Matt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b016f7b1-3efc-40ca-8eb4-502ae60f1d33",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Transforming CRS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384c5eeb",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Handling and transforming CRS - Matt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d489c6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Dataset loading - aka load all datasets to tables - Matt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165d277b",
   "metadata": {},
   "source": [
    "# Loading datasets into WherobotsDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b71f7ea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T01:55:45.137550Z",
     "iopub.status.busy": "2025-10-01T01:55:45.137306Z",
     "iopub.status.idle": "2025-10-01T01:55:45.140655Z",
     "shell.execute_reply": "2025-10-01T01:55:45.140155Z",
     "shell.execute_reply.started": "2025-10-01T01:55:45.137530Z"
    },
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "prefix = 's3://wherobots-examples/gdea-course-data/raw-data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "958cc513-5569-413a-9ede-3f5c0e586a4f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T01:55:52.056645Z",
     "iopub.status.busy": "2025-10-01T01:55:52.056434Z",
     "iopub.status.idle": "2025-10-01T01:55:52.063339Z",
     "shell.execute_reply": "2025-10-01T01:55:52.062895Z",
     "shell.execute_reply.started": "2025-10-01T01:55:52.056629Z"
    }
   },
   "outputs": [],
   "source": [
    "def check_invalid_geometries(df: DataFrame, geom_col: str = \"geom\", reason_col: str = \"why_invalid\") -> int:\n",
    "    df_with_reason = df.withColumn(reason_col, ST_IsValidReason(col(geom_col)))\n",
    "    # cache to avoid recomputation if you inspect reasons later\n",
    "    df_with_reason.cache()\n",
    "    invalid_count = df_with_reason.filter(~ST_IsValid(col(geom_col))).count()\n",
    "    print(f\"✅ Checked geometries — found {invalid_count} invalid geometries.\")\n",
    "    return invalid_count\n",
    "\n",
    "def fix_invalid_geometries(df: DataFrame, invalid_count: int, geom_col: str = \"geom\") -> DataFrame:\n",
    "    if invalid_count > 1:\n",
    "        print(f\"🔧 Attempting to fix {invalid_count} invalid geometries...\")\n",
    "        return df.withColumn(\n",
    "            geom_col,\n",
    "            when(~ST_IsValid(col(geom_col)), ST_MakeValid(col(geom_col))).otherwise(col(geom_col))\n",
    "        )\n",
    "    else:\n",
    "        print(\"⚡ Only one invalid geometry (or none). Skipping automated fix.\")\n",
    "        return df\n",
    "\n",
    "# --- driver program ---\n",
    "def process_geometries(\n",
    "    df: DataFrame,\n",
    "    geom_col: str = \"geom\",\n",
    "    attempt_fix: bool = True,\n",
    "    split_on_fail: bool = True\n",
    "):\n",
    "    \"\"\"\n",
    "    Runs validity check -> optional repair -> optional split.\n",
    "    Returns either:\n",
    "      - {\"df\": corrected_df}  when all geometries valid after repair (or none invalid)\n",
    "      - {\"valid_df\": ..., \"invalid_df\": ...} when some invalid remain and split_on_fail=True\n",
    "    \"\"\"\n",
    "    # 1) Initial check\n",
    "    invalid_count = check_invalid_geometries(df, geom_col=geom_col)\n",
    "\n",
    "    if invalid_count == 0:\n",
    "        print(\"✅ All geometries are valid.\")\n",
    "        return {\"df\": df}  # nothing to do\n",
    "\n",
    "    # 2) Attempt repair (only changes rows that are invalid per your earlier contract)\n",
    "    if attempt_fix:\n",
    "        df_fixed = fix_invalid_geometries(df, invalid_count, geom_col=geom_col)\n",
    "        remaining_invalid_count = df_fixed.filter(~ST_IsValid(col(geom_col))).count()\n",
    "        print(f\"🔎 After fixing, {remaining_invalid_count} invalid geometries remain.\")\n",
    "    \n",
    "        if remaining_invalid_count == 0:\n",
    "            print(\"✅ All geometries are valid after fixing.\")\n",
    "            return {\"df\": df_fixed}\n",
    "        elif split_on_fail:\n",
    "            print(\"⚠️ Some invalid geometries remain — splitting dataset.\")\n",
    "            valid_df = df_fixed.filter(ST_IsValid(col(geom_col)))\n",
    "            invalid_df = df_fixed.filter(~ST_IsValid(col(geom_col)))\n",
    "            print(f\"✅ Split complete: {valid_df.count()} valid / {invalid_df.count()} invalid.\")\n",
    "            return {\"valid_df\": valid_df, \"invalid_df\": invalid_df}\n",
    "        else:\n",
    "            print(\"⚠️ Some invalid geometries remain, returning best-effort fixed DataFrame.\")\n",
    "            return {\"df\": df_fixed}\n",
    "    \n",
    "    # If no fix attempt, just split if requested\n",
    "    if split_on_fail:\n",
    "        print(\"⚠️ Skipping fix — splitting into valid and invalid.\")\n",
    "        valid_df = df.filter(ST_IsValid(col(geom_col)))\n",
    "        invalid_df = df.filter(~ST_IsValid(col(geom_col)))\n",
    "        print(f\"✅ Split complete: {valid_df.count()} valid / {invalid_df.count()} invalid.\")\n",
    "        return {\"valid_df\": valid_df, \"invalid_df\": invalid_df}\n",
    "    \n",
    "    print(\"⚠️ Invalid geometries found but no fix or split requested. Returning original DataFrame.\")\n",
    "    return {\"df\": df}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e34a2ada-50cd-4bc8-aeb9-4ffc5aa5ef0e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T01:55:53.660177Z",
     "iopub.status.busy": "2025-10-01T01:55:53.659965Z",
     "iopub.status.idle": "2025-10-01T01:55:55.891021Z",
     "shell.execute_reply": "2025-10-01T01:55:55.890129Z",
     "shell.execute_reply.started": "2025-10-01T01:55:53.660161Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# FEMA Flood Hazard Areas\n",
    "fld_hazard_area = sedona.read.format('shapefile').load(f'{prefix}' + '53033C_20250330/S_FLD_HAZ_AR.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fc0531c3-b464-426a-b6c2-d4cc2243f0bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-30T18:43:28.218185Z",
     "iopub.status.busy": "2025-09-30T18:43:28.217980Z",
     "iopub.status.idle": "2025-09-30T18:43:32.280299Z",
     "shell.execute_reply": "2025-09-30T18:43:32.279877Z",
     "shell.execute_reply.started": "2025-09-30T18:43:28.218170Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/30 18:43:28 WARN CacheManager: Asked to cache already cached data.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Checked geometries — found 15 invalid geometries.\n",
      "🔧 Attempting to fix 15 invalid geometries...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 37:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔎 After fixing, 0 invalid geometries remain.\n",
      "✅ All geometries are valid after fixing.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "result = process_geometries(fld_hazard_area, geom_col=\"geometry\", attempt_fix=True, split_on_fail=True)\n",
    "\n",
    "if \"df\" in result:\n",
    "    df_final = result[\"df\"]  # all valid (either already valid or successfully repaired)\n",
    "else:\n",
    "    valid_df = result[\"valid_df\"]\n",
    "    invalid_df = result[\"invalid_df\"]\n",
    "    # handle invalids (e.g., export for manual review)(fld_hazard_area, 'geometry')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "654d631f-9d25-47ee-be41-b2480ef9018b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-30T18:43:35.309951Z",
     "iopub.status.busy": "2025-09-30T18:43:35.309729Z",
     "iopub.status.idle": "2025-09-30T18:43:37.712061Z",
     "shell.execute_reply": "2025-09-30T18:43:37.711481Z",
     "shell.execute_reply.started": "2025-09-30T18:43:35.309935Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_final.writeTo(f\"wherobots.{database}.fema_flood_zones_bronze\").createOrReplace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "80c6b264-da6b-4a4e-bc4f-4e1ad86896da",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-30T18:43:37.712886Z",
     "iopub.status.busy": "2025-09-30T18:43:37.712755Z",
     "iopub.status.idle": "2025-09-30T18:43:38.893841Z",
     "shell.execute_reply": "2025-09-30T18:43:38.893503Z",
     "shell.execute_reply.started": "2025-09-30T18:43:37.712870Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# King County Generalized Land Use Data\n",
    "gen_land_use = sedona.read.format('shapefile').load(f'{prefix}' + 'General_Land_Use_Final_Dataset/General_Land_Use_Final_Dataset.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "149331f3-509d-4c99-bc02-60320289d3ca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-30T18:43:38.894456Z",
     "iopub.status.busy": "2025-09-30T18:43:38.894338Z",
     "iopub.status.idle": "2025-09-30T18:44:48.020339Z",
     "shell.execute_reply": "2025-09-30T18:44:48.019878Z",
     "shell.execute_reply.started": "2025-09-30T18:43:38.894444Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Checked geometries — found 2987 invalid geometries.\n",
      "🔧 Attempting to fix 2987 invalid geometries...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 46:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔎 After fixing, 0 invalid geometries remain.\n",
      "✅ All geometries are valid after fixing.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "result = process_geometries(gen_land_use, geom_col=\"geometry\", attempt_fix=True, split_on_fail=True)\n",
    "\n",
    "if \"df\" in result:\n",
    "    df_final = result[\"df\"]  # all valid (either already valid or successfully repaired)\n",
    "else:\n",
    "    valid_df = result[\"valid_df\"]\n",
    "    invalid_df = result[\"invalid_df\"]\n",
    "    # handle invalids (e.g., export for manual review)(fld_hazard_area, 'geometry')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "19728834-c881-41e2-8449-8737f47343cb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-30T18:44:48.021152Z",
     "iopub.status.busy": "2025-09-30T18:44:48.021004Z",
     "iopub.status.idle": "2025-09-30T18:45:31.819161Z",
     "shell.execute_reply": "2025-09-30T18:45:31.818717Z",
     "shell.execute_reply.started": "2025-09-30T18:44:48.021140Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "valid_df.writeTo(f\"wherobots.{database}.gen_land_use_bronze\").createOrReplace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0bd61b53-a501-4144-a796-cf3d8859163a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-30T18:50:41.225455Z",
     "iopub.status.busy": "2025-09-30T18:50:41.225255Z",
     "iopub.status.idle": "2025-09-30T18:50:42.449358Z",
     "shell.execute_reply": "2025-09-30T18:50:42.448917Z",
     "shell.execute_reply.started": "2025-09-30T18:50:41.225439Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# King County Sherrif Patrol Districts\n",
    "sherrif_districts = sedona.read.format('shapefile').load(f'{prefix}' + 'King_County_Sheriff_Patrol_Districts___patrol_districts_area/King_County_Sheriff_Patrol_Districts___patrol_districts_area.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "542baeea-ee1b-4386-bd8b-6556666dbe62",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-30T18:51:21.944727Z",
     "iopub.status.busy": "2025-09-30T18:51:21.944526Z",
     "iopub.status.idle": "2025-09-30T18:51:29.889185Z",
     "shell.execute_reply": "2025-09-30T18:51:29.888755Z",
     "shell.execute_reply.started": "2025-09-30T18:51:21.944713Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 51:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Checked geometries — found 0 invalid geometries.\n",
      "✅ All geometries are valid.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "result = process_geometries(sherrif_districts, geom_col=\"geometry\", attempt_fix=True, split_on_fail=True)\n",
    "\n",
    "if \"df\" in result:\n",
    "    df_final = result[\"df\"]  # all valid (either already valid or successfully repaired)\n",
    "else:\n",
    "    valid_df = result[\"valid_df\"]\n",
    "    invalid_df = result[\"invalid_df\"]\n",
    "    # handle invalids (e.g., export for manual review)(fld_hazard_area, 'geometry')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6ab6769b-9897-4635-ac15-450d10cf6b2f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-30T18:51:47.848794Z",
     "iopub.status.busy": "2025-09-30T18:51:47.848561Z",
     "iopub.status.idle": "2025-09-30T18:51:57.211178Z",
     "shell.execute_reply": "2025-09-30T18:51:57.210175Z",
     "shell.execute_reply.started": "2025-09-30T18:51:47.848779Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_final.writeTo(f\"wherobots.{database}.sherrif_districts_bronze\").createOrReplace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "38a27b67-613c-4793-99c2-a0d8c82efa23",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-30T18:51:57.211971Z",
     "iopub.status.busy": "2025-09-30T18:51:57.211859Z",
     "iopub.status.idle": "2025-09-30T18:51:57.772699Z",
     "shell.execute_reply": "2025-09-30T18:51:57.771921Z",
     "shell.execute_reply.started": "2025-09-30T18:51:57.211959Z"
    }
   },
   "outputs": [],
   "source": [
    "offense_reports = sedona.read.format('csv').load(f'{prefix}' + 'KCSO_Offense_Reports__2020_to_Present_20250923.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f367d35a-e5e0-4a94-a5ea-4542a2030e93",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-30T18:52:03.463579Z",
     "iopub.status.busy": "2025-09-30T18:52:03.463372Z",
     "iopub.status.idle": "2025-09-30T18:52:06.601453Z",
     "shell.execute_reply": "2025-09-30T18:52:06.600785Z",
     "shell.execute_reply.started": "2025-09-30T18:52:03.463565Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "offense_reports.writeTo(f\"wherobots.{database}.offense_reports_bronze\").createOrReplace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a01f89e6-6075-4a93-9703-4cc040292382",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-30T18:53:16.841249Z",
     "iopub.status.busy": "2025-09-30T18:53:16.841042Z",
     "iopub.status.idle": "2025-09-30T18:53:17.291763Z",
     "shell.execute_reply": "2025-09-30T18:53:17.291106Z",
     "shell.execute_reply.started": "2025-09-30T18:53:16.841235Z"
    }
   },
   "outputs": [],
   "source": [
    "# King County Bike Lanes\n",
    "bike_lanes = sedona.read.format('shapefile').load(f'{prefix}' + 'Metro_Transportation_Network_(TNET)_in_King_County_for_Bicycle_Mode___trans_network_bike_line/Metro_Transportation_Network_(TNET)_in_King_County_for_Bicycle_Mode___trans_network_bike_line.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "09b6ddae-a8d2-4e7b-9d1a-f2467088f197",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-30T18:53:22.718728Z",
     "iopub.status.busy": "2025-09-30T18:53:22.718497Z",
     "iopub.status.idle": "2025-09-30T18:53:34.581955Z",
     "shell.execute_reply": "2025-09-30T18:53:34.581521Z",
     "shell.execute_reply.started": "2025-09-30T18:53:22.718711Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 59:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Checked geometries — found 0 invalid geometries.\n",
      "✅ All geometries are valid.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "result = process_geometries(bike_lanes, geom_col=\"geometry\", attempt_fix=True, split_on_fail=True)\n",
    "\n",
    "if \"df\" in result:\n",
    "    df_final = result[\"df\"]  # all valid (either already valid or successfully repaired)\n",
    "else:\n",
    "    valid_df = result[\"valid_df\"]\n",
    "    invalid_df = result[\"invalid_df\"]\n",
    "    # handle invalids (e.g., export for manual review)(fld_hazard_area, 'geometry')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ac773691-ecaa-46f4-8b01-8407b9059015",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-30T18:53:41.887141Z",
     "iopub.status.busy": "2025-09-30T18:53:41.886938Z",
     "iopub.status.idle": "2025-09-30T18:53:52.253464Z",
     "shell.execute_reply": "2025-09-30T18:53:52.253064Z",
     "shell.execute_reply.started": "2025-09-30T18:53:41.887125Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_final.writeTo(f\"wherobots.{database}.bike_lanes_bronze\").createOrReplace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5929ef8b-0ff9-4178-8d12-f529297945de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-30T18:56:46.162425Z",
     "iopub.status.busy": "2025-09-30T18:56:46.162221Z",
     "iopub.status.idle": "2025-09-30T18:56:46.677148Z",
     "shell.execute_reply": "2025-09-30T18:56:46.676716Z",
     "shell.execute_reply.started": "2025-09-30T18:56:46.162410Z"
    }
   },
   "outputs": [],
   "source": [
    "# FEMA National Risk Index\n",
    "fema_nri = sedona.read.format('shapefile').load(f'{prefix}' + 'NRI_Shapefile_CensusTracts/NRI_Shapefile_CensusTracts.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6b6568ac-091f-4675-a7f7-92e40da1f2b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-30T19:01:47.267173Z",
     "iopub.status.busy": "2025-09-30T19:01:47.266954Z",
     "iopub.status.idle": "2025-09-30T19:02:14.940079Z",
     "shell.execute_reply": "2025-09-30T19:02:14.939623Z",
     "shell.execute_reply.started": "2025-09-30T19:01:47.267158Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/30 19:01:47 WARN CacheManager: Asked to cache already cached data.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Checked geometries — found 82 invalid geometries.\n",
      "🔧 Attempting to fix 82 invalid geometries...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 75:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔎 After fixing, 0 invalid geometries remain.\n",
      "✅ All geometries are valid after fixing.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "result = process_geometries(fema_nri, geom_col=\"geometry\", attempt_fix=True, split_on_fail=True)\n",
    "\n",
    "if \"df\" in result:\n",
    "    df_final = result[\"df\"]  # all valid (either already valid or successfully repaired)\n",
    "else:\n",
    "    valid_df = result[\"valid_df\"]\n",
    "    invalid_df = result[\"invalid_df\"]\n",
    "    # handle invalids (e.g., export for manual review)(fld_hazard_area, 'geometry')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "35e82d32-ce21-42b3-8589-48cc16998186",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-30T19:02:14.940892Z",
     "iopub.status.busy": "2025-09-30T19:02:14.940774Z",
     "iopub.status.idle": "2025-09-30T19:03:20.925754Z",
     "shell.execute_reply": "2025-09-30T19:03:20.925227Z",
     "shell.execute_reply.started": "2025-09-30T19:02:14.940880Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/30 19:19:03 WARN ExecutorPodsWatchSnapshotSource: Kubernetes client has been closed.\n"
     ]
    }
   ],
   "source": [
    "df_final.writeTo(f\"wherobots.{database}.fema_nri_bronze\").createOrReplace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab34dd50-0527-49b7-a969-fcea5859b708",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T01:56:06.329053Z",
     "iopub.status.busy": "2025-10-01T01:56:06.328820Z",
     "iopub.status.idle": "2025-10-01T01:56:07.664491Z",
     "shell.execute_reply": "2025-10-01T01:56:07.663892Z",
     "shell.execute_reply.started": "2025-10-01T01:56:06.329036Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# King County School Sites\n",
    "school_sites = sedona.read.format('shapefile').load(f'{prefix}' + 'School_Sites_in_King_County___schsite_point/School_Sites_in_King_County___schsite_point.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "350a2a7e-dcb4-4bee-aa3c-e01f4a9b404c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T01:56:15.672606Z",
     "iopub.status.busy": "2025-10-01T01:56:15.672388Z",
     "iopub.status.idle": "2025-10-01T01:56:24.296159Z",
     "shell.execute_reply": "2025-10-01T01:56:24.295753Z",
     "shell.execute_reply.started": "2025-10-01T01:56:15.672591Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Checked geometries — found 0 invalid geometries.\n",
      "✅ All geometries are valid.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "result = process_geometries(school_sites, geom_col=\"geometry\", attempt_fix=True, split_on_fail=True)\n",
    "\n",
    "if \"df\" in result:\n",
    "    df_final = result[\"df\"]  # all valid (either already valid or successfully repaired)\n",
    "else:\n",
    "    valid_df = result[\"valid_df\"]\n",
    "    invalid_df = result[\"invalid_df\"]\n",
    "    # handle invalids (e.g., export for manual review)(fld_hazard_area, 'geometry')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a913b76-ec1c-435b-b1e6-694d15ca157f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T01:56:27.584765Z",
     "iopub.status.busy": "2025-10-01T01:56:27.584545Z",
     "iopub.status.idle": "2025-10-01T01:56:37.144022Z",
     "shell.execute_reply": "2025-10-01T01:56:37.143565Z",
     "shell.execute_reply.started": "2025-10-01T01:56:27.584749Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_final.writeTo(f\"wherobots.{database}.school_sites_bronze\").createOrReplace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "517cf454-e658-48a1-955f-56092ad3bf7c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T02:01:21.104209Z",
     "iopub.status.busy": "2025-10-01T02:01:21.103988Z",
     "iopub.status.idle": "2025-10-01T02:01:21.555254Z",
     "shell.execute_reply": "2025-10-01T02:01:21.554572Z",
     "shell.execute_reply.started": "2025-10-01T02:01:21.104194Z"
    }
   },
   "outputs": [],
   "source": [
    "# Schools Report Card\n",
    "report_card = sedona.read. \\\n",
    "    format('csv'). \\\n",
    "    load(f'{prefix}' + 'Report_Card_Growth_for_2024-25_20250923.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2a37eaa3-fb75-433d-86ec-265d3f6273e3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T02:01:21.556105Z",
     "iopub.status.busy": "2025-10-01T02:01:21.555983Z",
     "iopub.status.idle": "2025-10-01T02:01:23.640309Z",
     "shell.execute_reply": "2025-10-01T02:01:23.639631Z",
     "shell.execute_reply.started": "2025-10-01T02:01:21.556092Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "report_card.writeTo(f\"wherobots.{database}.report_card_bronze\").createOrReplace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0907b912-285b-409d-9373-53bf49920b99",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T02:01:26.675366Z",
     "iopub.status.busy": "2025-10-01T02:01:26.675163Z",
     "iopub.status.idle": "2025-10-01T02:01:27.062354Z",
     "shell.execute_reply": "2025-10-01T02:01:27.061843Z",
     "shell.execute_reply.started": "2025-10-01T02:01:26.675351Z"
    }
   },
   "outputs": [],
   "source": [
    "# Seismic Hazards\n",
    "seismic_hazards = sedona.read. \\\n",
    "    format('shapefile'). \\\n",
    "    load(f'{prefix}' + 'Seismic_Hazards___seism_area/Seismic_Hazards___seism_area.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "faea4cf1-016f-4e15-82fb-9d5872a384f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T02:01:27.157905Z",
     "iopub.status.busy": "2025-10-01T02:01:27.157686Z",
     "iopub.status.idle": "2025-10-01T02:01:27.267155Z",
     "shell.execute_reply": "2025-10-01T02:01:27.266778Z",
     "shell.execute_reply.started": "2025-10-01T02:01:27.157889Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Checked geometries — found 0 invalid geometries.\n",
      "✅ All geometries are valid.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/01 02:01:27 WARN CacheManager: Asked to cache already cached data.\n"
     ]
    }
   ],
   "source": [
    "result = process_geometries(seismic_hazards, geom_col=\"geometry\", attempt_fix=True, split_on_fail=True)\n",
    "\n",
    "if \"df\" in result:\n",
    "    df_final = result[\"df\"]  # all valid (either already valid or successfully repaired)\n",
    "else:\n",
    "    valid_df = result[\"valid_df\"]\n",
    "    invalid_df = result[\"invalid_df\"]\n",
    "    # handle invalids (e.g., export for manual review)(fld_hazard_area, 'geometry')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "93d2d673-ce35-48bb-82d1-36d8c432a392",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T02:01:32.425360Z",
     "iopub.status.busy": "2025-10-01T02:01:32.425141Z",
     "iopub.status.idle": "2025-10-01T02:01:39.065892Z",
     "shell.execute_reply": "2025-10-01T02:01:39.065461Z",
     "shell.execute_reply.started": "2025-10-01T02:01:32.425345Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_final.writeTo(f\"wherobots.{database}.seismic_hazards_bronze\").createOrReplace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "848c1509-27a0-42ec-b8cc-5b6b58c3287e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T02:01:54.001000Z",
     "iopub.status.busy": "2025-10-01T02:01:54.000781Z",
     "iopub.status.idle": "2025-10-01T02:01:54.370608Z",
     "shell.execute_reply": "2025-10-01T02:01:54.370015Z",
     "shell.execute_reply.started": "2025-10-01T02:01:54.000985Z"
    }
   },
   "outputs": [],
   "source": [
    "# Census Block Groups\n",
    "block_groups = sedona.read. \\\n",
    "    format('shapefile'). \\\n",
    "    load(f'{prefix}' + 'tl_2024_53_bg/tl_2024_53_bg.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9f47e586-1703-4dc1-abc8-7ac226fd1f4e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T02:01:54.828552Z",
     "iopub.status.busy": "2025-10-01T02:01:54.828333Z",
     "iopub.status.idle": "2025-10-01T02:01:55.155472Z",
     "shell.execute_reply": "2025-10-01T02:01:55.154911Z",
     "shell.execute_reply.started": "2025-10-01T02:01:54.828536Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/01 02:01:54 WARN CacheManager: Asked to cache already cached data.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Checked geometries — found 0 invalid geometries.\n",
      "✅ All geometries are valid.\n"
     ]
    }
   ],
   "source": [
    "result = process_geometries(block_groups, geom_col=\"geometry\", attempt_fix=True, split_on_fail=True)\n",
    "\n",
    "if \"df\" in result:\n",
    "    df_final = result[\"df\"]  # all valid (either already valid or successfully repaired)\n",
    "else:\n",
    "    valid_df = result[\"valid_df\"]\n",
    "    invalid_df = result[\"invalid_df\"]\n",
    "    # handle invalids (e.g., export for manual review)(fld_hazard_area, 'geometry')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "218d1da0-4276-45c0-8838-ec202257210a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T02:01:57.831933Z",
     "iopub.status.busy": "2025-10-01T02:01:57.831713Z",
     "iopub.status.idle": "2025-10-01T02:01:59.943150Z",
     "shell.execute_reply": "2025-10-01T02:01:59.942683Z",
     "shell.execute_reply.started": "2025-10-01T02:01:57.831917Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_final.writeTo(f\"wherobots.{database}.block_groups_bronze\").createOrReplace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "103d5f5f-ddf8-47f4-a5f1-5fb77496bac2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T02:04:26.679350Z",
     "iopub.status.busy": "2025-10-01T02:04:26.679142Z",
     "iopub.status.idle": "2025-10-01T02:04:30.963106Z",
     "shell.execute_reply": "2025-10-01T02:04:30.962334Z",
     "shell.execute_reply.started": "2025-10-01T02:04:26.679334Z"
    }
   },
   "outputs": [],
   "source": [
    "# Census CSVs\n",
    "median_age = sedona.read. \\\n",
    "    format('csv'). \\\n",
    "    load(f'{prefix}' + 'ACSDT5Y2023.B01002_2025-09-19T105233/ACSDT5Y2023.B01002-Data.csv')\n",
    "\n",
    "median_age.writeTo(f\"wherobots.{database}.median_age_bronze\").createOrReplace()\n",
    "\n",
    "total_pop = sedona.read. \\\n",
    "    format('csv'). \\\n",
    "    load(f'{prefix}' + 'ACSDT5Y2023.B01003_2025-09-19T105050/ACSDT5Y2023.B01003-Data.csv')\n",
    "\n",
    "total_pop.writeTo(f\"wherobots.{database}.total_pop_bronze\").createOrReplace()\n",
    "\n",
    "median_income = sedona.read. \\\n",
    "    format('csv'). \\\n",
    "    load(f'{prefix}' + 'ACSDT5Y2023.B19013_2025-09-19T105253/ACSDT5Y2023.B19013-Data.csv')\n",
    "\n",
    "total_pop.writeTo(f\"wherobots.{database}.median_income_bronze\").createOrReplace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "19610058-ff7d-41fe-807d-7132fec745bb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T02:05:21.706104Z",
     "iopub.status.busy": "2025-10-01T02:05:21.705885Z",
     "iopub.status.idle": "2025-10-01T02:05:22.175298Z",
     "shell.execute_reply": "2025-10-01T02:05:22.174689Z",
     "shell.execute_reply.started": "2025-10-01T02:05:21.706089Z"
    }
   },
   "outputs": [],
   "source": [
    "# Tranist Routes\n",
    "transit_routes = sedona.read. \\\n",
    "    format('shapefile'). \\\n",
    "    load(f'{prefix}' + 'Transit_Routes_for_King_County_Metro___transitroute_line/Transit_Routes_for_King_County_Metro___transitroute_line.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "38fb1716-891b-4882-8cab-6195492dea79",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T02:05:22.748048Z",
     "iopub.status.busy": "2025-10-01T02:05:22.747837Z",
     "iopub.status.idle": "2025-10-01T02:05:29.591537Z",
     "shell.execute_reply": "2025-10-01T02:05:29.590942Z",
     "shell.execute_reply.started": "2025-10-01T02:05:22.748032Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 44:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Checked geometries — found 0 invalid geometries.\n",
      "✅ All geometries are valid.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "result = process_geometries(transit_routes, geom_col=\"geometry\", attempt_fix=True, split_on_fail=True)\n",
    "\n",
    "if \"df\" in result:\n",
    "    df_final = result[\"df\"]  # all valid (either already valid or successfully repaired)\n",
    "else:\n",
    "    valid_df = result[\"valid_df\"]\n",
    "    invalid_df = result[\"invalid_df\"]\n",
    "    # handle invalids (e.g., export for manual review)(fld_hazard_area, 'geometry')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "668665e0-8ca0-47c1-ad0b-6de4aa338d49",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T02:05:29.592416Z",
     "iopub.status.busy": "2025-10-01T02:05:29.592285Z",
     "iopub.status.idle": "2025-10-01T02:05:37.070529Z",
     "shell.execute_reply": "2025-10-01T02:05:37.069752Z",
     "shell.execute_reply.started": "2025-10-01T02:05:29.592403Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_final.writeTo(f\"wherobots.{database}.transit_routes_bronze\").createOrReplace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1b97e87d-4079-4f92-8e35-af08b6d0df95",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T02:06:29.667339Z",
     "iopub.status.busy": "2025-10-01T02:06:29.667128Z",
     "iopub.status.idle": "2025-10-01T02:06:30.214351Z",
     "shell.execute_reply": "2025-10-01T02:06:30.213676Z",
     "shell.execute_reply.started": "2025-10-01T02:06:29.667324Z"
    }
   },
   "outputs": [],
   "source": [
    "# Transit Stops\n",
    "transit_stops = sedona.read. \\\n",
    "    format('shapefile'). \\\n",
    "    load(f'{prefix}' + 'Transit_Stops_for_King_County_Metro___transitstop_point/Transit_Stops_for_King_County_Metro___transitstop_point.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d1b1bb6e-8d28-4247-b3c5-4d7f26630e85",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T02:06:30.318869Z",
     "iopub.status.busy": "2025-10-01T02:06:30.318651Z",
     "iopub.status.idle": "2025-10-01T02:06:38.452927Z",
     "shell.execute_reply": "2025-10-01T02:06:38.452445Z",
     "shell.execute_reply.started": "2025-10-01T02:06:30.318854Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 50:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Checked geometries — found 0 invalid geometries.\n",
      "✅ All geometries are valid.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "result = process_geometries(transit_stops, geom_col=\"geometry\", attempt_fix=True, split_on_fail=True)\n",
    "\n",
    "if \"df\" in result:\n",
    "    df_final = result[\"df\"]  # all valid (either already valid or successfully repaired)\n",
    "else:\n",
    "    valid_df = result[\"valid_df\"]\n",
    "    invalid_df = result[\"invalid_df\"]\n",
    "    # handle invalids (e.g., export for manual review)(fld_hazard_area, 'geometry')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "52c05ba8-9750-41b0-a695-a37fa1b83be5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T02:14:34.379212Z",
     "iopub.status.busy": "2025-10-01T02:14:34.379005Z",
     "iopub.status.idle": "2025-10-01T02:14:41.484285Z",
     "shell.execute_reply": "2025-10-01T02:14:41.483743Z",
     "shell.execute_reply.started": "2025-10-01T02:14:34.379197Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_final.writeTo(f\"wherobots.{database}.transit_stops_bronze\").createOrReplace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3ab603b1-706d-403f-9ba7-bc49820bb224",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T02:15:03.968609Z",
     "iopub.status.busy": "2025-10-01T02:15:03.968392Z",
     "iopub.status.idle": "2025-10-01T02:15:04.526132Z",
     "shell.execute_reply": "2025-10-01T02:15:04.525166Z",
     "shell.execute_reply.started": "2025-10-01T02:15:03.968594Z"
    }
   },
   "outputs": [],
   "source": [
    "# Water Bodies\n",
    "water_bodies = sedona.read. \\\n",
    "    format('shapefile'). \\\n",
    "    load(f'{prefix}' + 'Waterbodies_with_History_and_Jurisdictional_detail___wtrbdy_det_area/Waterbodies_with_History_and_Jurisdictional_detail___wtrbdy_det_area.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "86c0cfe3-14a5-458e-8fd8-69794cc07565",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T02:15:11.617341Z",
     "iopub.status.busy": "2025-10-01T02:15:11.617137Z",
     "iopub.status.idle": "2025-10-01T02:15:28.653662Z",
     "shell.execute_reply": "2025-10-01T02:15:28.653017Z",
     "shell.execute_reply.started": "2025-10-01T02:15:11.617326Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Checked geometries — found 5 invalid geometries.\n",
      "🔧 Attempting to fix 5 invalid geometries...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 60:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔎 After fixing, 0 invalid geometries remain.\n",
      "✅ All geometries are valid after fixing.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "result = process_geometries(water_bodies, geom_col=\"geometry\", attempt_fix=True, split_on_fail=True)\n",
    "\n",
    "if \"df\" in result:\n",
    "    df_final = result[\"df\"]  # all valid (either already valid or successfully repaired)\n",
    "else:\n",
    "    valid_df = result[\"valid_df\"]\n",
    "    invalid_df = result[\"invalid_df\"]\n",
    "    # handle invalids (e.g., export for manual review)(fld_hazard_area, 'geometry')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c96768e9-9c12-4946-ba3e-7d1bb29c271e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T02:15:29.753277Z",
     "iopub.status.busy": "2025-10-01T02:15:29.753044Z",
     "iopub.status.idle": "2025-10-01T02:15:38.216698Z",
     "shell.execute_reply": "2025-10-01T02:15:38.216262Z",
     "shell.execute_reply.started": "2025-10-01T02:15:29.753261Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_final.writeTo(f\"wherobots.{database}.water_bodies_bronze\").createOrReplace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "09f651b2-135e-483a-b376-1722e1b69d08",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T02:17:29.993248Z",
     "iopub.status.busy": "2025-10-01T02:17:29.993038Z",
     "iopub.status.idle": "2025-10-01T02:17:30.401920Z",
     "shell.execute_reply": "2025-10-01T02:17:30.401183Z",
     "shell.execute_reply.started": "2025-10-01T02:17:29.993233Z"
    }
   },
   "outputs": [],
   "source": [
    "# Wildfire Polygons\n",
    "wildfires = sedona.read. \\\n",
    "    format('shapefile'). \\\n",
    "    load(f'{prefix}' + 'Wildfires_1878_2019_Polygon_Data/Shapefile/US_Wildfires_1878_2019.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3b275ad2-5cc6-4825-a123-7179a0b56928",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T02:17:33.479757Z",
     "iopub.status.busy": "2025-10-01T02:17:33.479527Z",
     "iopub.status.idle": "2025-10-01T02:18:15.500275Z",
     "shell.execute_reply": "2025-10-01T02:18:15.499882Z",
     "shell.execute_reply.started": "2025-10-01T02:17:33.479741Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Checked geometries — found 1203 invalid geometries.\n",
      "🔧 Attempting to fix 1203 invalid geometries...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 70:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔎 After fixing, 0 invalid geometries remain.\n",
      "✅ All geometries are valid after fixing.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "result = process_geometries(wildfires, geom_col=\"geometry\", attempt_fix=True, split_on_fail=True)\n",
    "\n",
    "if \"df\" in result:\n",
    "    df_final = result[\"df\"]  # all valid (either already valid or successfully repaired)\n",
    "else:\n",
    "    valid_df = result[\"valid_df\"]\n",
    "    invalid_df = result[\"invalid_df\"]\n",
    "    # handle invalids (e.g., export for manual review)(fld_hazard_area, 'geometry')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f036a2b1-a97d-4731-ad41-fd73d87296f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T02:18:15.501046Z",
     "iopub.status.busy": "2025-10-01T02:18:15.500903Z",
     "iopub.status.idle": "2025-10-01T02:18:38.684490Z",
     "shell.execute_reply": "2025-10-01T02:18:38.683764Z",
     "shell.execute_reply.started": "2025-10-01T02:18:15.501032Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_final.writeTo(f\"wherobots.{database}.wildfires_bronze\").createOrReplace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dddd5cd5-16a2-408b-942e-0e8a86126ef4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-30T19:23:25.942472Z",
     "iopub.status.busy": "2025-09-30T19:23:25.942278Z",
     "iopub.status.idle": "2025-09-30T19:23:25.944744Z",
     "shell.execute_reply": "2025-09-30T19:23:25.944439Z",
     "shell.execute_reply.started": "2025-09-30T19:23:25.942459Z"
    }
   },
   "outputs": [],
   "source": [
    "# Wildfire Rasters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "278f504d-8689-4c71-bbb1-bf9d2697dab7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-30T19:23:42.708892Z",
     "iopub.status.busy": "2025-09-30T19:23:42.708684Z",
     "iopub.status.idle": "2025-09-30T19:23:42.710991Z",
     "shell.execute_reply": "2025-09-30T19:23:42.710638Z",
     "shell.execute_reply.started": "2025-09-30T19:23:42.708878Z"
    }
   },
   "outputs": [],
   "source": [
    "# Elevation\n",
    "\n",
    "# https://s3.opengeohub.org/global/edtm/gedtm_rf_m_30m_s_20060101_20151231_go_epsg.4326.3855_v20250611.tif\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8270605c-65ae-4222-8975-554d17b03b53",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T02:26:56.493984Z",
     "iopub.status.busy": "2025-10-01T02:26:56.493765Z",
     "iopub.status.idle": "2025-10-01T02:26:57.043200Z",
     "shell.execute_reply": "2025-10-01T02:26:57.042663Z",
     "shell.execute_reply.started": "2025-10-01T02:26:56.493969Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Geocoded Schools\n",
    "schools = sedona.read. \\\n",
    "    format('geojson'). \\\n",
    "    load(f'{prefix}' + 'Washington_State_Public_Schools_GeoCoded.geojson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "34c889f2-9210-4c03-b6a1-5357929ea913",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T02:29:02.255826Z",
     "iopub.status.busy": "2025-10-01T02:29:02.255609Z",
     "iopub.status.idle": "2025-10-01T02:29:02.410215Z",
     "shell.execute_reply": "2025-10-01T02:29:02.409726Z",
     "shell.execute_reply.started": "2025-10-01T02:29:02.255809Z"
    }
   },
   "outputs": [],
   "source": [
    "schools = schools \\\n",
    "    .withColumn(\"geometry\", expr(\"geometry\")) \\\n",
    "    .withColumn(\"AYPCode\", expr(\"properties['AYPCode']\")) \\\n",
    "    .withColumn(\"CongressionalDistrict\", expr(\"properties['CongressionalDistrict']\")) \\\n",
    "    .withColumn(\"County\", expr(\"properties['County']\")) \\\n",
    "    .withColumn(\"ESDCode\", expr(\"properties['ESDCode']\")) \\\n",
    "    .withColumn(\"ESDName\", expr(\"properties['ESDName']\")) \\\n",
    "    .withColumn(\"Email\", expr(\"properties['Email']\")) \\\n",
    "    .withColumn(\"GeoCoded_X\", expr(\"properties['GeoCoded_X']\")) \\\n",
    "    .withColumn(\"GeoCoded_Y\", expr(\"properties['GeoCoded_Y']\")) \\\n",
    "    .withColumn(\"GradeCategory\", expr(\"properties['GradeCategory']\")) \\\n",
    "    .withColumn(\"HighestGrade\", expr(\"properties['HighestGrade']\")) \\\n",
    "    .withColumn(\"LEACode\", expr(\"properties['LEACode']\")) \\\n",
    "    .withColumn(\"LEAName\", expr(\"properties['LEAName']\")) \\\n",
    "    .withColumn(\"LegislativeDistrict\", expr(\"properties['LegislativeDistrict']\")) \\\n",
    "    .withColumn(\"LowestGrade\", expr(\"properties['LowestGrade']\")) \\\n",
    "    .withColumn(\"MailingAddress\", expr(\"properties['MailingAddress']\")) \\\n",
    "    .withColumn(\"NCES_X\", expr(\"properties['NCES_X']\")) \\\n",
    "    .withColumn(\"NCES_Y\", expr(\"properties['NCES_Y']\")) \\\n",
    "    .withColumn(\"Phone\", expr(\"properties['Phone']\")) \\\n",
    "    .withColumn(\"Principal\", expr(\"properties['Principal']\")) \\\n",
    "    .withColumn(\"School\", expr(\"properties['School']\")) \\\n",
    "    .withColumn(\"SchoolCategory\", expr(\"properties['SchoolCategory']\")) \\\n",
    "    .withColumn(\"SchoolCode\", expr(\"properties['SchoolCode']\")) \\\n",
    "    .withColumn(\"SingleAddress\", expr(\"properties['SingleAddress']\")) \\\n",
    "    .drop(\"properties\").drop(\"type\") \\\n",
    "    .drop(\"_corrupt_record\").drop(\"type\") \\\n",
    "    .drop(\"type\").drop(\"type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f8ce1324-82c6-4300-8389-70e3b4935044",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T02:30:15.910373Z",
     "iopub.status.busy": "2025-10-01T02:30:15.910144Z",
     "iopub.status.idle": "2025-10-01T02:30:15.914331Z",
     "shell.execute_reply": "2025-10-01T02:30:15.913846Z",
     "shell.execute_reply.started": "2025-10-01T02:30:15.910357Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- geometry: geometry (nullable = true)\n",
      " |-- AYPCode: string (nullable = true)\n",
      " |-- CongressionalDistrict: string (nullable = true)\n",
      " |-- County: string (nullable = true)\n",
      " |-- ESDCode: long (nullable = true)\n",
      " |-- ESDName: string (nullable = true)\n",
      " |-- Email: string (nullable = true)\n",
      " |-- GeoCoded_X: double (nullable = true)\n",
      " |-- GeoCoded_Y: double (nullable = true)\n",
      " |-- GradeCategory: string (nullable = true)\n",
      " |-- HighestGrade: string (nullable = true)\n",
      " |-- LEACode: long (nullable = true)\n",
      " |-- LEAName: string (nullable = true)\n",
      " |-- LegislativeDistrict: string (nullable = true)\n",
      " |-- LowestGrade: string (nullable = true)\n",
      " |-- MailingAddress: string (nullable = true)\n",
      " |-- NCES_X: double (nullable = true)\n",
      " |-- NCES_Y: double (nullable = true)\n",
      " |-- Phone: string (nullable = true)\n",
      " |-- Principal: string (nullable = true)\n",
      " |-- School: string (nullable = true)\n",
      " |-- SchoolCategory: string (nullable = true)\n",
      " |-- SchoolCode: long (nullable = true)\n",
      " |-- SingleAddress: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schools.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "004fd9c5-5174-4642-9472-f06df63ed46e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T02:29:05.808796Z",
     "iopub.status.busy": "2025-10-01T02:29:05.808592Z",
     "iopub.status.idle": "2025-10-01T02:29:07.004511Z",
     "shell.execute_reply": "2025-10-01T02:29:07.003843Z",
     "shell.execute_reply.started": "2025-10-01T02:29:05.808781Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/01 02:29:06 ERROR TaskSetManager: Task 0 in stage 84.0 failed 4 times; aborting job\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o494.count.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 84.0 failed 4 times, most recent failure: Lost task 0.3 in stage 84.0 (TID 1698) (10.1.31.162 executor 8): java.lang.NullPointerException: Cannot invoke \"org.apache.spark.unsafe.types.UTF8String.toString()\" because the return value of \"org.apache.spark.sql.catalyst.InternalRow.getUTF8String(int)\" is null\n\tat org.apache.spark.sql.catalyst.InternalRow.getString(InternalRow.scala:35)\n\tat org.apache.spark.sql.sedona_sql.io.geojson.GeoJSONUtils$.$anonfun$convertGeoJsonToGeometry$1(GeoJSONUtils.scala:111)\n\tat org.apache.spark.sql.sedona_sql.io.geojson.GeoJSONUtils$.$anonfun$convertGeoJsonToGeometry$1$adapted(GeoJSONUtils.scala:109)\n\tat scala.collection.ArrayOps$.foreach$extension(ArrayOps.scala:1328)\n\tat org.apache.spark.sql.sedona_sql.io.geojson.GeoJSONUtils$.convertGeoJsonToGeometry(GeoJSONUtils.scala:109)\n\tat org.apache.spark.sql.sedona_sql.io.geojson.GeoJSONFileFormat.$anonfun$buildReader$3(GeoJSONFileFormat.scala:181)\n\tat scala.collection.Iterator$$anon$9.next(Iterator.scala:577)\n\tat scala.collection.Iterator$$anon$9.next(Iterator.scala:577)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.next(FileScanRDD.scala:199)\n\tat scala.collection.Iterator$$anon$9.next(Iterator.scala:577)\n\tat scala.collection.Iterator$$anon$9.next(Iterator.scala:577)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:90)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:80)\n\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:290)\n\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:287)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:224)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1597)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1524)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1588)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:379)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.immutable.List.foreach(List.scala:333)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:437)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\nCaused by: java.lang.NullPointerException\n\tat org.apache.spark.sql.catalyst.InternalRow.getString(InternalRow.scala:35)\n\tat org.apache.spark.sql.sedona_sql.io.geojson.GeoJSONUtils$.$anonfun$convertGeoJsonToGeometry$1(GeoJSONUtils.scala:111)\n\tat org.apache.spark.sql.sedona_sql.io.geojson.GeoJSONUtils$.$anonfun$convertGeoJsonToGeometry$1$adapted(GeoJSONUtils.scala:109)\n\tat scala.collection.ArrayOps$.foreach$extension(ArrayOps.scala:1328)\n\tat org.apache.spark.sql.sedona_sql.io.geojson.GeoJSONUtils$.convertGeoJsonToGeometry(GeoJSONUtils.scala:109)\n\tat org.apache.spark.sql.sedona_sql.io.geojson.GeoJSONFileFormat.$anonfun$buildReader$3(GeoJSONFileFormat.scala:181)\n\tat scala.collection.Iterator$$anon$9.next(Iterator.scala:577)\n\tat scala.collection.Iterator$$anon$9.next(Iterator.scala:577)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.next(FileScanRDD.scala:199)\n\tat scala.collection.Iterator$$anon$9.next(Iterator.scala:577)\n\tat scala.collection.Iterator$$anon$9.next(Iterator.scala:577)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:90)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:80)\n\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:290)\n\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:287)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:224)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1597)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1524)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1588)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:379)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPy4JJavaError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[55]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m result = \u001b[43mprocess_geometries\u001b[49m\u001b[43m(\u001b[49m\u001b[43mschools\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgeom_col\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgeometry\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattempt_fix\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit_on_fail\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mdf\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m result:\n\u001b[32m      4\u001b[39m     df_final = result[\u001b[33m\"\u001b[39m\u001b[33mdf\u001b[39m\u001b[33m\"\u001b[39m]  \u001b[38;5;66;03m# all valid (either already valid or successfully repaired)\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 34\u001b[39m, in \u001b[36mprocess_geometries\u001b[39m\u001b[34m(df, geom_col, attempt_fix, split_on_fail)\u001b[39m\n\u001b[32m     27\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     28\u001b[39m \u001b[33;03mRuns validity check -> optional repair -> optional split.\u001b[39;00m\n\u001b[32m     29\u001b[39m \u001b[33;03mReturns either:\u001b[39;00m\n\u001b[32m     30\u001b[39m \u001b[33;03m  - {\"df\": corrected_df}  when all geometries valid after repair (or none invalid)\u001b[39;00m\n\u001b[32m     31\u001b[39m \u001b[33;03m  - {\"valid_df\": ..., \"invalid_df\": ...} when some invalid remain and split_on_fail=True\u001b[39;00m\n\u001b[32m     32\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m# 1) Initial check\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m invalid_count = \u001b[43mcheck_invalid_geometries\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgeom_col\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeom_col\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m invalid_count == \u001b[32m0\u001b[39m:\n\u001b[32m     37\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m✅ All geometries are valid.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 5\u001b[39m, in \u001b[36mcheck_invalid_geometries\u001b[39m\u001b[34m(df, geom_col, reason_col)\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# cache to avoid recomputation if you inspect reasons later\u001b[39;00m\n\u001b[32m      4\u001b[39m df_with_reason.cache()\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m invalid_count = \u001b[43mdf_with_reason\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfilter\u001b[49m\u001b[43m(\u001b[49m\u001b[43m~\u001b[49m\u001b[43mST_IsValid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgeom_col\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m✅ Checked geometries — found \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minvalid_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m invalid geometries.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m invalid_count\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/spark/python/pyspark/sql/dataframe.py:1240\u001b[39m, in \u001b[36mDataFrame.count\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1217\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcount\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28mint\u001b[39m:\n\u001b[32m   1218\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Returns the number of rows in this :class:`DataFrame`.\u001b[39;00m\n\u001b[32m   1219\u001b[39m \n\u001b[32m   1220\u001b[39m \u001b[33;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1238\u001b[39m \u001b[33;03m    3\u001b[39;00m\n\u001b[32m   1239\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1240\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/wherobots/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1316\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1317\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1318\u001b[39m     args_command +\\\n\u001b[32m   1319\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1321\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1323\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/spark/python/pyspark/errors/exceptions/captured.py:179\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdeco\u001b[39m(*a: Any, **kw: Any) -> Any:\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    180\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    181\u001b[39m         converted = convert_exception(e.java_exception)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/wherobots/lib/python3.11/site-packages/py4j/protocol.py:326\u001b[39m, in \u001b[36mget_return_value\u001b[39m\u001b[34m(answer, gateway_client, target_id, name)\u001b[39m\n\u001b[32m    324\u001b[39m value = OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[32m2\u001b[39m:], gateway_client)\n\u001b[32m    325\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[32m1\u001b[39m] == REFERENCE_TYPE:\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[32m    327\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    328\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name), value)\n\u001b[32m    329\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    330\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[32m    331\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    332\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name, value))\n",
      "\u001b[31mPy4JJavaError\u001b[39m: An error occurred while calling o494.count.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 84.0 failed 4 times, most recent failure: Lost task 0.3 in stage 84.0 (TID 1698) (10.1.31.162 executor 8): java.lang.NullPointerException: Cannot invoke \"org.apache.spark.unsafe.types.UTF8String.toString()\" because the return value of \"org.apache.spark.sql.catalyst.InternalRow.getUTF8String(int)\" is null\n\tat org.apache.spark.sql.catalyst.InternalRow.getString(InternalRow.scala:35)\n\tat org.apache.spark.sql.sedona_sql.io.geojson.GeoJSONUtils$.$anonfun$convertGeoJsonToGeometry$1(GeoJSONUtils.scala:111)\n\tat org.apache.spark.sql.sedona_sql.io.geojson.GeoJSONUtils$.$anonfun$convertGeoJsonToGeometry$1$adapted(GeoJSONUtils.scala:109)\n\tat scala.collection.ArrayOps$.foreach$extension(ArrayOps.scala:1328)\n\tat org.apache.spark.sql.sedona_sql.io.geojson.GeoJSONUtils$.convertGeoJsonToGeometry(GeoJSONUtils.scala:109)\n\tat org.apache.spark.sql.sedona_sql.io.geojson.GeoJSONFileFormat.$anonfun$buildReader$3(GeoJSONFileFormat.scala:181)\n\tat scala.collection.Iterator$$anon$9.next(Iterator.scala:577)\n\tat scala.collection.Iterator$$anon$9.next(Iterator.scala:577)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.next(FileScanRDD.scala:199)\n\tat scala.collection.Iterator$$anon$9.next(Iterator.scala:577)\n\tat scala.collection.Iterator$$anon$9.next(Iterator.scala:577)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:90)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:80)\n\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:290)\n\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:287)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:224)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1597)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1524)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1588)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:379)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.immutable.List.foreach(List.scala:333)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:437)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\nCaused by: java.lang.NullPointerException\n\tat org.apache.spark.sql.catalyst.InternalRow.getString(InternalRow.scala:35)\n\tat org.apache.spark.sql.sedona_sql.io.geojson.GeoJSONUtils$.$anonfun$convertGeoJsonToGeometry$1(GeoJSONUtils.scala:111)\n\tat org.apache.spark.sql.sedona_sql.io.geojson.GeoJSONUtils$.$anonfun$convertGeoJsonToGeometry$1$adapted(GeoJSONUtils.scala:109)\n\tat scala.collection.ArrayOps$.foreach$extension(ArrayOps.scala:1328)\n\tat org.apache.spark.sql.sedona_sql.io.geojson.GeoJSONUtils$.convertGeoJsonToGeometry(GeoJSONUtils.scala:109)\n\tat org.apache.spark.sql.sedona_sql.io.geojson.GeoJSONFileFormat.$anonfun$buildReader$3(GeoJSONFileFormat.scala:181)\n\tat scala.collection.Iterator$$anon$9.next(Iterator.scala:577)\n\tat scala.collection.Iterator$$anon$9.next(Iterator.scala:577)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.next(FileScanRDD.scala:199)\n\tat scala.collection.Iterator$$anon$9.next(Iterator.scala:577)\n\tat scala.collection.Iterator$$anon$9.next(Iterator.scala:577)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:90)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:80)\n\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:290)\n\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:287)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:224)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1597)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1524)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1588)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:379)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n"
     ]
    }
   ],
   "source": [
    "result = process_geometries(schools, geom_col=\"geometry\", attempt_fix=True, split_on_fail=True)\n",
    "\n",
    "if \"df\" in result:\n",
    "    df_final = result[\"df\"]  # all valid (either already valid or successfully repaired)\n",
    "else:\n",
    "    valid_df = result[\"valid_df\"]\n",
    "    invalid_df = result[\"invalid_df\"]\n",
    "    # handle invalids (e.g., export for manual review)(fld_hazard_area, 'geometry')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ac3d21-b73c-475c-b3f0-e8254de09b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.writeTo(f\"wherobots.{database}.schools_bronze\").createOrReplace()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
